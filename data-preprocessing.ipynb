{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neroma Kossi : 3A ENSAE, AS-DS\n",
    "> Projet du cours d'éléments logiciels pour le traitement de données massives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our porject consists of parallelising the Latent Dirichlet Allocation (**LDA**) algorithm. The base paper is [https://www.semanticscholar.org/paper/PLDA%3A-Parallel-Latent-Dirichlet-Allocation-for-Wang-Bai/376ffb536c3dc5675e9ab875b10b9c4a1437da5d](PLDA, a parallel gibbs sampling based algorithm).\n",
    "\n",
    "The main idea is  to run concurrent Gibb's sampling algorithms. This could be done via a distributed framework like MPI or mapReduce, we will be considering the last one in this project. Pyspark will be the standard library for the mapReduce architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,  SparkContext  # Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math ops\n",
    "import os, shutil, json #File ops\n",
    "import pickle as pkl # Serialiser\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utilities saved into custom modules\n",
    "\n",
    "from nlp import preprocessAndGetTokens\n",
    "from fileUtils import load, pickleLoader, dump, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_memory = '1g' # Max memory available for the driver\n",
    "executor_memory = '1g' # Max memory by executor\n",
    "# We have to set those params before instantiating the SparkContext, other It would be too late\n",
    "pyspark_submit_args = ' --driver-memory {0} --executor-memory {1} pyspark-shell'\\\n",
    "                                .format(driver_memory, executor_memory)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([\n",
    "     ('spark.app.name', 'pLDA'), \n",
    "     ('spark.master', 'local[*]'), # the number of cores is set to max\n",
    "    ('spark.scheduler.mode', 'FAIR'),\n",
    "    ('spark.files.maxPartitionBytes', '500Mo')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(conf = conf) # Here we create the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.files.maxPartitionBytes', '500Mo'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'pLDA'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.driver.host', '192.168.0.41'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.app.id', 'local-1549573274837'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.port', '43347')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data pre-processing\n",
    "\n",
    "Our dataset is made of many abstracts of research papers in Computer Science, Neuroscience, and Biomedical. It is available at no cost on https://labs.semanticscholar.org/corpus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load the data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadPaperAbstract(docstr, trunc = 1500):\n",
    "    \"\"\"Convert the paper into json, keep only paper's id and first 1500 chars of its Abstract.\"\"\"\n",
    "    \n",
    "    doc = json.loads(docstr)\n",
    "    return (doc[\"id\"], doc[\"paperAbstract\"][:trunc], doc[\"title\"][:trunc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPaperAbstract(abstract):\n",
    "    \"\"\"This is a wrapper that calls the preprocessAndGetTokens function. The latest function will apply \n",
    "    some basic nlp tehchnics on the paper's abstract : lowercase-isation, stopwords removing, stemming...\"\"\"\n",
    "    \n",
    "    return np.array(preprocessAndGetTokens(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPartitions = 10 # Set the number of partitions, this is important as our Gibbs sampler is designed to \n",
    "                # lunch one sampler per partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Let's read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.textFile(\"sample-S2-records\")\\\n",
    "            .repartition(nbPartitions)\\\n",
    "                .map(LoadPaperAbstract)\n",
    "# data = spark.textFile('/home/nerk/Downloads/s2-corpus-46.gz')\\\n",
    "#         .repartition(nbPartitions)\\\n",
    "#                 .map(LoadPaperAbstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.getNumPartitions() # Same as assigned above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sample(False, 1/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.9 ms, sys: 136 µs, total: 19.1 ms\n",
      "Wall time: 236 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1500, 0, 0, 762, 0, 0, 628, 603, 360, 0]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "data.map(lambda x : len(x[1])).take(10) # A look on the abstracts' length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 ms, sys: 550 µs, total: 20.4 ms\n",
      "Wall time: 144 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e',\n",
       "  'Primary debulking surgery (PDS) has historically been the standard treatment for advanced ovarian cancer. Recent data appear to support a paradigm shift toward neoadjuvant chemotherapy with interval debulking surgery (NACT-IDS). We hypothesized that stage IV ovarian cancer patients would likely benefit from NACT-IDS by achieving similar outcomes with less morbidity. Patients with stage IV epithelial ovarian cancer who underwent primary treatment between January 1, 1995 and December 31, 2007, were identified. Data were retrospectively extracted. Each patient record was evaluated to subclassify stage IV disease according to the sites of tumor dissemination at the time of diagnosis. The Kaplan–Meier method was used to compare overall survival (OS) data. A total of 242 newly diagnosed stage IV epithelial ovarian cancer patients were included in the final analysis; 176 women (73%) underwent PDS, 45 (18%) NACT-IDS, and 21 (9%) chemotherapy only. The frequency of achieving complete resection to no residual disease was significantly higher in patients with NACT-IDS versus PDS (27% vs. 7.5%; P\\xa0<\\xa00.001). When compared to women treated with NACT-IDS, women with PDS had longer admissions (12 vs. 8\\xa0days; P\\xa0=\\xa00.01), more frequent intensive care unit admissions (12% vs. 0%; P\\xa0=\\xa00.01), and a trend toward a higher rate of postoperative complications (27% vs. 15%; P\\xa0=\\xa00.08). The patients who received only chemotherapy had a median OS of 23\\xa0months, compared to 33\\xa0months in the NACT-IDS group an',\n",
       "  'Primary Debulking Surgery Versus Neoadjuvant Chemotherapy in Stage IV Ovarian Cancer'),\n",
       " ('4c61478345166be0d917854bd5e5f42a6ade2362',\n",
       "  '',\n",
       "  'Lipid transport function of lipoproteins in flying insects.')]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data.take(2)  # A sampler of our datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"matrix/docTitles/\") :\n",
    "    shutil.rmtree(\"matrix/docTitles/\")\n",
    "data.map(lambda x :  {x[0]:x[1]}).saveAsPickleFile(\"matrix/docTitles/\") # Save doc titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11 ms, sys: 141 µs, total: 11.2 ms\n",
      "Wall time: 1.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Now we do all the preprocessing, and save the dataset\n",
    "folder = \"corpus/sample-S2-records/\"\n",
    "if os.path.exists(folder) :\n",
    "    shutil.rmtree(folder)\n",
    "    \n",
    "data = data.mapValues(processPaperAbstract)\\\n",
    "                    .filter(lambda x : len(x[1]) > 0)\\\n",
    "                    \n",
    "data.saveAsPickleFile(\"corpus/sample-S2-records/\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e',\n",
       "  array(['unit', 'analysi', 'compar', 'includ', 'histor', 'decemb',\n",
       "         'surgeri', 'subclassifi', 'onli', 'paradigm', 'chemotherapi',\n",
       "         'treatment', 'diagnosi', 'cancer', 'trend', 'complet', 'januari',\n",
       "         'appear', 'admiss', 'recent', 'final', 'toward', 'complic',\n",
       "         'accord', 'receiv', 'resect', 'ovarian', 'frequenc', 'os',\n",
       "         'support', 'treat', 'method', 'outcom', 'use', 'versus', 'patient',\n",
       "         'postop', 'extract', 'evalu', 'time', 'standard', 'diagnos',\n",
       "         'tumor', 'nactid', 'retrospect', 'achiev', 'overal', 'higher',\n",
       "         'kaplanmei', 'vs', 'record', 'advanc', 'diseas', 'iv', 'pds',\n",
       "         'shift', 'site', 'stage', 'neoadjuv', 'identifi', 'intens', 'like',\n",
       "         'epitheli', 'residu', 'less', 'benefit', 'hypothes', 'total',\n",
       "         'longer', 'group', 'underw', 'month', 'surviv', 'median',\n",
       "         'dissemin', 'data', 'signific', 'interv', 'morbid', 'similar',\n",
       "         'newli', 'frequent', 'would', 'women', 'rate', 'care', 'primari',\n",
       "         'day', 'debulk'], dtype='<U12'))]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1) # A sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here, our dataset is in the primal format (docId, docContent). Next, we will assign a random topic to each word in a document. We will also need to build the Vocaulary and the set of the documents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3. Building the vocabulary and the set of docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading and partionning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus = spark.pickleFile(\"corpus/sample-S2-records/\" ).repartition(nbPartitions)\n",
    "corpus = spark.pickleFile(\"corpus/corpus-46/part-00000\" ).repartition(nbPartitions)\n",
    "#         .union(spark.pickleFile(\"corpus/corpus-46/part-00001\" ))\n",
    "corpus.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus2 = corpus.map(lambda x  : randomPartitionner(x, nbPartitions))\\\n",
    "#             .partitionBy(nbPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.4 ms, sys: 1.09 ms, total: 12.5 ms\n",
      "Wall time: 4.05 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be8aa262e6d0c7122ba3e4b619da264c00ae15b9',\n",
       "  array(['approxim', 'analysi', 'ethnic', 'men', 'physic', 'old', 'unclear',\n",
       "         'louisiana', 'state', 'pressur', 'intak', 'brfss', 'postmenopaus',\n",
       "         'three', 'determin', 'condit', 'alcohol', 'may', 'cholesterol',\n",
       "         'veget', 'result', 'moder', 'blood', 'tennesse', 'screen',\n",
       "         'relationship', 'factor', 'activ', 'onethird', 'effect', 'report',\n",
       "         'studi', 'system', 'behavior', 'whether', 'daili', 'treat',\n",
       "         'method', 'four', 'diabet', 'tobacco', 'adjust', 'use', 'femal',\n",
       "         'crosssect', 'set', 'diagnos', 'race', 'surveil', 'howev',\n",
       "         'premenopaus', 'obes', 'control', 'half', 'conclus', 'smoke',\n",
       "         'weight', 'recommend', 'present', 'educ', 'relat', 'symptom',\n",
       "         'purpos', 'comorbid', 'status', 'indic', 'regress', 'fruit',\n",
       "         'larg', 'data', 'nevada', 'signific', 'overweight', 'year',\n",
       "         'women', 'across', 'care', 'primari', 'logist', 'size', 'michigan',\n",
       "         'risk', 'age', 'assess', 'multipl', 'high'], dtype='<U12'))]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.1. Build the vocabularies (one per partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'builder' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/builder.py'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, builder\n",
    "importlib.reload(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import makeVocabularies, makeVocabulariesFolder, getUniqueWords, getUniqueWords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeVocabulariesFolder() # Instantiate the vocabularies' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.4 ms, sys: 83.7 ms, total: 141 ms\n",
      "Wall time: 27.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Here we compute the set of unique words. But a word can sometimes very long, \n",
    "# we will assign (in next steps) to each word a number ranging from 0 to V-1, where V == size of ours vocabs\n",
    "uniqueWordsByPartition = corpus.mapPartitionsWithIndex(getUniqueWords).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Partition': '00', 'ndocs': 3250, 'nvocabs': 31522},\n",
       " {'Partition': '01', 'ndocs': 3260, 'nvocabs': 32318},\n",
       " {'Partition': '02', 'ndocs': 3260, 'nvocabs': 32495},\n",
       " {'Partition': '03', 'ndocs': 3260, 'nvocabs': 33301},\n",
       " {'Partition': '04', 'ndocs': 3260, 'nvocabs': 33233}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents & words per partition\n",
    "\n",
    "L = [{\"Partition\": \"%02d\"%i, \"ndocs\": len(x[0]), \"nvocabs\": len(x[1])} for x, i \n",
    "             in zip(uniqueWordsByPartition, range(nbPartitions))  ]\n",
    "L[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal docs : 32569 \n"
     ]
    }
   ],
   "source": [
    "print(\"Totoal docs : %d \"%sum(l[\"ndocs\"] for l in L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary 0 successfully built\n",
      "Vocabulary 1 successfully built\n",
      "Vocabulary 2 successfully built\n",
      "Vocabulary 3 successfully built\n",
      "Vocabulary 4 successfully built\n",
      "Vocabulary 5 successfully built\n",
      "Vocabulary 6 successfully built\n",
      "Vocabulary 7 successfully built\n",
      "Vocabulary 8 successfully built\n",
      "Vocabulary 9 successfully built\n",
      "\n",
      " Global vocabulary  built too\n",
      "CPU times: user 6.3 s, sys: 672 ms, total: 6.98 s\n",
      "Wall time: 5.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Here we build the vocabularies, one per partition\n",
    "\n",
    "makeVocabularies([ w[1] for w in  uniqueWordsByPartition]) # Build and save the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "del uniqueWordsByPartition # free up somme memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.2. Make docMaps :  the set of all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import builder, importlib\n",
    "# importlib.relaod(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import makeDocsMaps, makeDocsMapsFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeDocsMapsFolder() # Instantiate the documents' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.09 ms, sys: 123 µs, total: 9.21 ms\n",
      "Wall time: 574 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['docMap 0 successfully built',\n",
       " 'docMap 1 successfully built',\n",
       " 'docMap 2 successfully built',\n",
       " 'docMap 3 successfully built',\n",
       " 'docMap 4 successfully built',\n",
       " 'docMap 5 successfully built',\n",
       " 'docMap 6 successfully built',\n",
       " 'docMap 7 successfully built',\n",
       " 'docMap 8 successfully built',\n",
       " 'docMap 9 successfully built']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus.mapPartitionsWithIndex(makeDocsMaps).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3.3. Test if vocabularies and docMaps are correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabSize = len(load(\"matrix/vocabulary/vocabAll\"))\n",
    "# print(\"vocabSize : %d \"%vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndocs00 = len(load(\"matrix/docsMap/docs__0000__\"))\n",
    "# print(\"Number of docs in docs00 : %d\"%ndocs00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As voacabularies & docMaps was successfully built, let's load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.15 s, sys: 39.5 ms, total: 1.19 s\n",
      "Wall time: 1.19 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabAll = load(\"matrix/vocabulary/vocabAll\")\n",
    "\n",
    "vocabs = [load(\"matrix/vocabulary/vocab__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Vocab :  151812\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in Vocab : \", len(vocabAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.7 ms, sys: 177 µs, total: 25.9 ms\n",
      "Wall time: 43.3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from builder import loadDocsAll\n",
    "docsAll = loadDocsAll(nbPartitions)\n",
    "\n",
    "docs = [load(\"matrix/docsMap/docs__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3250, 3260] [31522, 32318]\n",
      "CPU times: user 0 ns, sys: 1.71 ms, total: 1.71 ms\n",
      "Wall time: 1.06 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nbDocs = list(map(len, docs))\n",
    "nbVocabs = list(map(len, vocabs))\n",
    "print(nbDocs[:2], nbVocabs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prepare the data for the training step\n",
    ">Encode corpus and add topics : using ids instead of doc full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Encode corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import builder, importlib\n",
    "importlib.reload(builder)\n",
    "from builder  import encodeAddTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.22 ms, sys: 0 ns, total: 7.22 ms\n",
      "Wall time: 57.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('be8aa262e6d0c7122ba3e4b619da264c00ae15b9',\n",
       "  array(['approxim', 'analysi', 'ethnic', 'men', 'physic', 'old', 'unclear',\n",
       "         'louisiana', 'state', 'pressur', 'intak', 'brfss', 'postmenopaus',\n",
       "         'three', 'determin', 'condit', 'alcohol', 'may', 'cholesterol',\n",
       "         'veget', 'result', 'moder', 'blood', 'tennesse', 'screen',\n",
       "         'relationship', 'factor', 'activ', 'onethird', 'effect', 'report',\n",
       "         'studi', 'system', 'behavior', 'whether', 'daili', 'treat',\n",
       "         'method', 'four', 'diabet', 'tobacco', 'adjust', 'use', 'femal',\n",
       "         'crosssect', 'set', 'diagnos', 'race', 'surveil', 'howev',\n",
       "         'premenopaus', 'obes', 'control', 'half', 'conclus', 'smoke',\n",
       "         'weight', 'recommend', 'present', 'educ', 'relat', 'symptom',\n",
       "         'purpos', 'comorbid', 'status', 'indic', 'regress', 'fruit',\n",
       "         'larg', 'data', 'nevada', 'signific', 'overweight', 'year',\n",
       "         'women', 'across', 'care', 'primari', 'logist', 'size', 'michigan',\n",
       "         'risk', 'age', 'assess', 'multipl', 'high'], dtype='<U12'))]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# The corpius is in full text again, let's change it in the next step\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTopics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can notice that all the words have been encoded into symbolic ids, topics  have been added too\n",
    "corpus2 = corpus.mapPartitionsWithIndex(lambda ind, part : encodeAddTopics(ind, part,docs[ind],\n",
    "                                                                           vocabs[ind], nbTopics), \n",
    "                                       preservesPartitioning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.6 s, sys: 506 ms, total: 25.1 s\n",
      "Wall time: 26.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  (2153, array([    2, 15724, 13033,  5618, 27042,  4616,  9594,  9125, 31045,\n",
       "           1042, 14542,  2022,  9139, 17210, 27572, 17212,  6891, 11348,\n",
       "            290, 15306, 17485, 15791, 17005, 31340, 29573, 17495, 21204,\n",
       "          29580, 13827, 29581, 19967, 30079,  9923, 16770, 11866, 16776,\n",
       "           7207, 10183,  5454,  8231, 31372, 17772,   349, 15103, 27135,\n",
       "           8245, 30390, 23937, 26646, 20745,  2829, 14883,   606, 21484,\n",
       "          31161,  8783, 31177,  9490, 22036,   421, 30690,  4552,  5287,\n",
       "          17614, 10760, 21799, 27228,  7066,  7589, 20103,  5828, 27500,\n",
       "          12255, 10531, 12748, 31499,  4823, 25997, 13490,  9089, 21349,\n",
       "          20878, 14750, 29989, 25515,  8879]), array([9, 6, 7, 1, 7, 9, 6, 7, 5, 5, 7, 1, 6, 5, 3, 2, 9, 4, 3, 5, 4, 8,\n",
       "          7, 2, 9, 1, 0, 8, 4, 2, 5, 5, 9, 1, 0, 5, 7, 3, 8, 9, 3, 5, 6, 4,\n",
       "          5, 0, 5, 8, 6, 8, 8, 3, 9, 2, 6, 0, 1, 9, 1, 8, 0, 7, 3, 6, 7, 2,\n",
       "          8, 1, 4, 4, 0, 3, 1, 3, 3, 0, 6, 5, 0, 9, 0, 2, 1, 8, 4, 4])))]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus2.take(1) # Just word's and doc's ids now, topics have been added too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Save the whole work for the next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileUtils import saveAsPickleFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 24.5 s, sys: 211 ms, total: 24.7 s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saveAsPickleFile(corpus2)\n",
    "if os.path.exists(\"initial_train\"):\n",
    "    shutil.rmtree(\"initial_train\")\n",
    "corpus2.saveAsPickleFile(\"initial_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is the end of the data preprocessing, the data is in the right format now and we can train our model. Let's sart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, corpus, corpus2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parallel LDA (mapReduce version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbVocabAll = len(vocabAll)\n",
    "alpha = 0.1\n",
    "beta = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builder import makeConfig, updateConfig, get_now\n",
    "# makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, model, builder, fileUtils\n",
    "importlib.reload(model)\n",
    "importlib.reload(builder)\n",
    "importlib.reload(fileUtils)\n",
    "from fileUtils import saveAsPickleFile\n",
    "from model import pldaMap0\n",
    "from builder import updateCountWordsAll, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pldaMap(0, 1, alpha, beta, len(vocabAll), nbTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = spark.pickleFile(\"pickle/\")\n",
    "# rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  (3209, array([14352, 26794,  6539, 25779, 16817, 18158, 30432,  4878, 25353,\n",
       "          28446, 14962, 20647,  3334, 19869,  1979,  2372,  4267, 25919,\n",
       "          21715, 28195, 17774, 15978, 29534,  1131, 29342, 20536,  1263,\n",
       "           4825, 27174,  6720, 10404, 28654, 27089,  7755, 28584, 20482,\n",
       "          21929, 14178, 26413, 32594, 22826,   502, 12037,  9509,  2975,\n",
       "           5851, 28311, 33085, 31120, 17305, 12981,  9543, 27103, 28075,\n",
       "          13975, 14374,  6536, 21439, 20559]), array([9, 6, 7, 1, 7, 9, 6, 7, 5, 5, 7, 1, 6, 5, 3, 2, 9, 4, 3, 5, 4, 8,\n",
       "          7, 2, 9, 1, 0, 8, 4, 2, 5, 5, 9, 1, 0, 5, 7, 3, 8, 9, 3, 5, 6, 4,\n",
       "          5, 0, 5, 8, 6, 8, 8, 3, 9, 2, 6, 0, 1, 9, 1])))]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.pickleFile(\"initial_train\")\n",
    "# (doc_id, doc_words, doc_topics) <--- the format\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 1518.7058153152466\n",
      "CPU times: user 26.1 s, sys: 1.39 s, total: 27.5 s\n",
      "Wall time: 25min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "rdd = spark.pickleFile(\"initial_train\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "# rdd = corpus2\n",
    "init(rdd, vocabs, nbDocs, nbVocabs, len(vocabAll), nbTopics)\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    rdd = rdd.mapPartitionsWithIndex(lambda ind, part : pldaMap0(ind, part, alpha, beta, nbVocabAll, nbTopics),\n",
    "                       preservesPartitioning= True )\n",
    "    saveAsPickleFile(rdd)\n",
    "    rdd = spark.pickleFile(\"pickle/\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "    updateCountWordsAll()\n",
    "print(\"Time : {}\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import initDocCounts, initWordCounts, initCountWordsAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import makeConfig, get_now, pldaMap, updateConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(25, array([ 886,    0,  802, 1284,    3,  535,  265, 1048,  589,  806,  193,\n",
       "          767,  269,  348,   52,  393,  428, 1083, 1298, 1019,  974,  553,\n",
       "         1022, 1167,  775,  376,   20,  982,  477,  872,  361,  819,   21,\n",
       "          649,  983, 1243,  907,  211,  367,  322,  961,  746,  486, 1104,\n",
       "          953, 1310,  289,  704,  660, 1240,  752, 1189, 1149,  529, 1156]), array([6, 2, 5, 3, 9, 8, 0, 9, 2, 8, 1, 2, 7, 2, 5, 0, 1, 2, 8, 5, 6, 4,\n",
       "         4, 4, 0, 7, 5, 3, 2, 9, 2, 9, 5, 8, 4, 0, 7, 7, 1, 5, 5, 2, 3, 8,\n",
       "         5, 2, 2, 2, 0, 4, 9, 2, 7, 1, 0]))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.pickleFile(\"initial_train\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervise(nrounds):\n",
    "    \"\"\"The superviser.\"\"\"\n",
    "    processes = [Process(target= pldaMap,\n",
    "            args = (ind, nrounds,alpha, beta, len(vocabAll), nbTopics)) for ind in range(nbPartitions)]\n",
    "    if  __name__ == \"__main__\":\n",
    "        # Run processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "    count = 0        \n",
    "    while count < nrounds :\n",
    "        allFree = True\n",
    "        for id_ in range(nbPartitions):\n",
    "            with open(\"configs/config__%04d__\"%id_, \"r\") as f :\n",
    "                slave = json.load(f)\n",
    "                if slave[\"state\"] == \"busy\":\n",
    "                    allFree = False\n",
    "                    \n",
    "        if allFree :\n",
    "            updateCountWordsAll()\n",
    "            updateConfig(id = \"all\", \n",
    "                         countWordsUpdated = {str(ind):True for ind in range(nbPartitions)}, time = get_now() )\n",
    "            count += 1\n",
    "        \n",
    "            \n",
    "        if not all([p.is_alive() for  p in processes]) :\n",
    "            for p in processes :\n",
    "                p.kill() \n",
    "            raise ValueError(\"Some process is died !!!!!!!!!!!!\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    for p in processes :\n",
    "                p.kill() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 2., 2., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.pickleFile(\"initial_train\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "rdd.mapPartitionsWithIndex(saveByPartition).collect()\n",
    "\n",
    "rdd.mapPartitionsWithIndex(lambda ind, part : initDocCounts(ind, part, nbDocs[ind], nbTopics)).collect()\n",
    "rdd.mapPartitionsWithIndex(lambda ind, part : initWordCounts(ind, part, nbVocabs[ind], nbTopics)).collect()\n",
    "\n",
    "initCountWordsAll(vocabs, len(vocabAll), nbTopics)\n",
    "\n",
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    }
   ],
   "source": [
    "makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master = Process(target = supervise, args = [10] )\n",
    "# master.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "master.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pldaMap(0, 1, alpha, beta, len(vocabAll), nbTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.88 ms, sys: 86 µs, total: 1.96 ms\n",
      "Wall time: 1.28 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "cl = 5\n",
    "subdoc = 1\n",
    "countDocs = load(\"matrix/countDocs/docs__%04d__\"%subdoc)\n",
    "countDocs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  3., ...,  0.,  0.,  3.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  1.,  2.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       ...,\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 9.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  4., 11., ...,  0.,  0.,  4.]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = countDocs.argmax(1)\n",
    "v = np.where(topics == cl)\n",
    "countDocs[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['c112e6cad39c90c3b8e4f3c7c99705a763272eb8', '2'],\n",
       "       ['a14a171c3627543bc0db4c5a07e9ee861acd2887', '12'],\n",
       "       ['ce2b170225dbcd0e453817a8efa9c5bfddc6edc2', '31'],\n",
       "       ['6c0a25fc368e0d8ae4680798b834e71e96c6820b', '38'],\n",
       "       ['fd6658ca16d647a564666bb947d678c763423cb1', '69'],\n",
       "       ['fa080d44b8906c268cca8ae15602d0bcd041a077', '70'],\n",
       "       ['75de42b2686d0337a8b58f4482ea72c18ee8f307', '71'],\n",
       "       ['ed82035983dd18f638d5d7689d0daaecf38cfbeb', '74'],\n",
       "       ['3618734a404b80e8adf020104a74e87450ac05d1', '82'],\n",
       "       ['bcab651e0fb36eece46bde097e1f4de1cf8081a5', '96']], dtype='<U40')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dks = np.array( list(docs[subdoc].items()))\n",
    "cluster = dks[v]\n",
    "cluster[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = spark.pickleFile(\"corpus/corpus-46/part-00000\" ).repartition(nbPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 7.83 ms, total: 23.8 ms\n",
      "Wall time: 5.85 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('32ebec9e3312f7a0b47b7ab346a91c8ceb1e7cae',\n",
       "  array(['secur', 'level', 'face', 'therefor', 'degrad', 'import', 'forc',\n",
       "         'anomali', 'avail', 'possibl', 'current', 'network', 'fall',\n",
       "         'propos', 'arriv', 'expect', 'determin', 'minim', 'monitor',\n",
       "         'flaw', 'experi', 'complet', 'region', 'isp', 'seri', 'show',\n",
       "         'reason', 'cope', 'connect', 'servic', 'syn', 'legitim', 'target',\n",
       "         'effect', 'profit', 'henc', 'provid', 'model', 'queue', 'note',\n",
       "         'daili', 'method', 'negat', 'nevertheless', 'client', 'tune',\n",
       "         'use', 'analyz', 'necessari', 'maximum', 'detect', 'howev',\n",
       "         'access', 'unwant', 'particular', 'fals', 'act', 'first', 'tcp',\n",
       "         'avoid', 'attack', 'poisson', 'deni', 'explain', 'realtim',\n",
       "         'intens', 'also', 'order', 'caus', 'synflood', 'paper', 'machin',\n",
       "         'accept', 'potenti', 'purpos', 'sever', 'length', 'fail',\n",
       "         'suitabl', 'correct', 'threat', 'requir', 'segment', 'solut',\n",
       "         'allow', 'probabl', 'signific', 'andor', 'would', 'rate', 'measur',\n",
       "         'decreas', 'aim', 'posit', 'host', 'actual', 'server', 'backlog',\n",
       "         'base'], dtype='<U12')),\n",
       " ('9108e100300ab2fff46108c69ef6aa123c957d1d',\n",
       "  array(['challeng', 'strategi', 'aspect', 'share', 'prove', 'lack',\n",
       "         'therefor', 'oper', 'manag', 'trove', 'compani', 'explor', 'larg',\n",
       "         'cultur', 'capabl', 'trust', 'dispers', 'differ', 'exploit',\n",
       "         'tacit', 'draw', 'superior', 'detail', 'articl', 'make',\n",
       "         'communic', 'global', 'innov', 'way', 'wilson', 'tool', 'team',\n",
       "         'expect', 'guidelin', 'inher', 'across', 'unearth', 'array', 'har',\n",
       "         'use', 'strong', 'replic', 'present', 'treasur', 'posit', 'appli',\n",
       "         'opportun', 'project', 'robust', 'harder', 'set', 'doz', 'problem',\n",
       "         'idea', 'research', 'reservoir', 'collabor', 'insead', 'context',\n",
       "         'tradit', 'singleloc', 'show', 'foster', 'recogn', 'skill',\n",
       "         'coloc', 'overcom', 'part', 'knowledg'], dtype='<U9')),\n",
       " ('40dd2dc9583fa4662d3b848f01ae50b5afdd5370',\n",
       "  array(['phenomenologicalcontextualist', 'phenomena', 'character', 'kind',\n",
       "         'form', 'resist', 'includ', 'psychoanalyt', 'rang', 'articl',\n",
       "         'comport', 'essenti', 'pathogenesi', 'therapeut', 'unconsci',\n",
       "         'entail', 'develop', 'outlin', 'trauma', 'perspect', 'appli',\n",
       "         'formul', 'wide', 'clinic', 'chang', 'emot', 'dwell', 'transfer'],\n",
       "        dtype='<U29')),\n",
       " ('3071789929c8826b27b8b490685d04e9e30c37f1',\n",
       "  array(['panel', 'nonclass', 'bondhold', 'artifici', 'member', 'uk',\n",
       "         'litig', 'background', 'group', 'associ', 'benchmark', 'otc',\n",
       "         'class', 'cost', 'brought', 'overthecount', 'synopsi', 'motion',\n",
       "         'plaintiff', 'underst', 'exchangebas', 'daili', 'financi',\n",
       "         'defend', 'sector', 'antitrust', 'suppress', 'rate', 'dismiss',\n",
       "         'also', 'libor', 'interbank', 'assembl', 'borrow', 'bank', 'offer',\n",
       "         'conspir', 'fourth', 'purport', 'trade', 'alleg', 'lead', 'servic',\n",
       "         'file', 'interest', 'london'], dtype='<U12')),\n",
       " ('6aebe283b52c9eeb3a0c24a9b7326e87d742995a',\n",
       "  array(['step', 'therefor', 'symmetr', 'ms', 'placement', 'improv',\n",
       "         'initi', 'timetocontact', 'biomechan', 'paradigm', 'margin',\n",
       "         'propos', 'retroreflect', 'right', 'determin', 'dynam', 'may',\n",
       "         'experi', 'comput', 'simul', 'recent', 'role', 'show', 'help',\n",
       "         'chang', 'bodi', 'result', 'greater', 'treadmil', 'fast',\n",
       "         'contact', 'construct', 'model', 'belt', 'twostat', 'maintain',\n",
       "         'mass', 'balanc', 'investig', 'splitbelt', 'spatiotempor',\n",
       "         'adjust', 'use', 'time', 'extrem', 'desir', 'adapt', 'asymmetr',\n",
       "         'stabil', 'human', 'consist', 'marker', 'character', 'foot',\n",
       "         'move', 'differ', 'gradual', 'angl', 'left', 'slow', 'experiment',\n",
       "         'common', 'coordin', 'walk', 'unequ', 'also', 'limb', 'metric',\n",
       "         'relat', 'although', 'understand', 'peopl', 'updat', 'lower',\n",
       "         'found', 'environ', 'length', 'explor', 'asymmetri', 'feet',\n",
       "         'adopt', 'novel', 'previous', 'year', 'center', 'kinemat',\n",
       "         'measur', 'particip', 'pattern', 'exponenti', 'decay',\n",
       "         'accomplish', 'influenc', 'yet', 'lead', 'drive', 'vertebr',\n",
       "         'base'], dtype='<U13')),\n",
       " ('d7b98bd2a4fc151f773c6f963ecbfc2a4d53b9e3',\n",
       "  array(['numer', 'ii', 'properti', 'therefor', 'oper', 'uniqu', 'ie',\n",
       "         'literatur', 'auxiliari', 'isomorph', 'consider', 'addit', 'proof',\n",
       "         'product', 'fttczt', 'condit', 'minim', 'treatment',\n",
       "         'jonssontarski', 'call', 'arbitrarili', 'seri', 'assert', 'appear',\n",
       "         'short', 'admiss', 'theorem', 'briefli', 'satisfi', 'result',\n",
       "         'suffici', 'princip', 'refin', 'theori', 'distinguish', 'close',\n",
       "         'factor', 'classic', 'algebra', 'general', 'assum', 'one',\n",
       "         'symbol', 'indecompos', 'system', 'element', 'ot', 'give', 'use',\n",
       "         'represent', 'set', 'zero', 'obvious', 'written', 'hold',\n",
       "         'consist', 'mention', 'thus', 'announc', 'differ', 'jet', 'follow',\n",
       "         'xpsa', 'detail', 'finit', 'common', 'identifi', 'ft', 'present',\n",
       "         'subject', 'jf', 'terminolog', 'wedderburnremakkrullschmidt',\n",
       "         'describ', 'much', 'local', 'maxim', 'rank', 'yield', 'group',\n",
       "         'direct', 'associ', 'wider', 'decomposit', 'class', 'function',\n",
       "         'larg', 'framework', 'preliminari', 'denot', 'binari', 'known',\n",
       "         'consequ', 'center', 'somewhat', 'develop', 'refer', 'exampl',\n",
       "         'shown', 'subgroup', 'shall', 'iii', 'crawley', 'mean', 'baer',\n",
       "         'mani', 'two', 'pt', 'actual', 'without'], dtype='<U27')),\n",
       " ('57d60802b3ae6e9dac84a156cc27a86526da65bc',\n",
       "  array(['electroderm', 'respect', 'driver', 'multisensori', 'consist',\n",
       "         'anfi', 'electromyogram', 'obtain', 'class', 'activ', 'methodolog',\n",
       "         'perform', 'interfac', 'follow', 'ten', 'system', 'central',\n",
       "         'electrocardiogram', 'preliminari', 'state', 'tenfold', 'data',\n",
       "         'approach', 'support', 'valid', 'respir', 'disappoint', 'propos',\n",
       "         'identifi', 'facial', 'rate', 'stress', 'infer', 'low', 'euphoria',\n",
       "         'modul', 'vector', 'condit', 'use', 'cross', 'neurofuzzi',\n",
       "         'subject', 'present', 'evalu', 'race', 'wearabl', 'carrac',\n",
       "         'adapt', 'comput', 'assess', 'simul', 'classif', 'paper', 'machin',\n",
       "         'svm', 'emot', 'high', 'achiev', 'svms', 'overal'], dtype='<U17')),\n",
       " ('46ece3987d6c57de925da556e7af9a347dcc0c46',\n",
       "  array(['bodi', 'xray', 'tissu', 'accid', 'collis', 'physic', 'studi',\n",
       "         'deal', 'chemic', 'graph', 'impos', 'frequent', 'condit', 'harm',\n",
       "         'patient', 'trauma', 'defin', 'traumat', 'caus', 'necess', 'risk',\n",
       "         'multipl', 'agent', 'expos', 'physician', 'introduct', 'injuri',\n",
       "         'prescrib', 'due'], dtype='<U9')),\n",
       " ('9fad768eb54089866374031626a08f63c166730e',\n",
       "  array(['challeng', 'respond', 'better', 'sensit', 'compar', 'import',\n",
       "         'peer', 'progress', 'initi', 'tool', 'prepar', 'student',\n",
       "         'nonjudgment', 'legal', 'teach', 'may', 'supervis', 'experi',\n",
       "         'complet', 'smallgroup', 'examin', 'design', 'role', 'chang',\n",
       "         'final', 'take', 'due', 'background', 'activ', 'medic', 'report',\n",
       "         'school', 'implement', 'largegroup', 'way', 'respons', 'medicin',\n",
       "         'issu', 'modul', 'survey', 'well', 'histori', 'faculti', 'analyz',\n",
       "         'sustain', 'evalu', 'aris', 'small', 'onlin', 'play', 'clinic',\n",
       "         'introduct', 'skill', 'overal', 'questionnair', 'enhanc', 'case',\n",
       "         'creat', 'descript', 'conclus', 'recruit', 'core', 'curriculum',\n",
       "         'sexualhistorytak', 'week', 'present', 'train', 'describ',\n",
       "         'question', 'address', 'area', 'part', 'group', 'introduc', 'larg',\n",
       "         'within', 'perform', 'requir', 'articl', 'icm', 'excel', 'success',\n",
       "         'basic', 'ndyear', 'anonym', 'situat', 'ask', 'cours', 'rate',\n",
       "         'develop', 'particip', 'topic', 'good', 'teacher', 'sexual',\n",
       "         'adequ', 'posit', 'facilit', 'mani', 'yet', 'physician', 'learn'],\n",
       "        dtype='<U16')),\n",
       " ('425671c00466108148238f5bbe7b1592ef4e4bd0',\n",
       "  array(['result', 'twostag', 'level', 'face', 'greater', 'new', 'control',\n",
       "         'deploy', 'rigor', 'shed', 'gain', 'sale', 'inventori', 'import',\n",
       "         'find', 'end', 'organiz', 'effect', 'profit', 'perform', 'object',\n",
       "         'adopt', 'uncertain', 'studi', 'decis', 'improv', 'shape',\n",
       "         'higher', 'bias', 'period', 'estim', 'achiev', 'allow', 'rfid',\n",
       "         'financi', 'method', 'sum', 'firm', 'endogen', 'low', 'adjust',\n",
       "         'like', 'use', 'aris', 'supplier', 'light', 'suppli', 'demand',\n",
       "         'examin', 'characterist', 'process', 'counterpart', 'heckman',\n",
       "         'relat', 'postadopt', 'show', 'role', 'high', 'drive', 'us',\n",
       "         'effici'], dtype='<U12'))]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus.filter(lambda x : np.isin(x[0], cluster[:, 0])).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = load(\"matrix/countWords/words__%04d__\"%1)\n",
    "countWords[countWords < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords[506]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(ind, part):\n",
    "    S = []\n",
    "    if ind == 2 :\n",
    "        for el in part :\n",
    "            if el[0] == 0:\n",
    "                S.append((ind, el))\n",
    "    return S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.pickleFile(\"pickle/\").partitionBy(nbPartitions).map(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (0, array([497,   3,  97,  98,   7,  99,   8, 240, 502, 352, 571, 390, 171,\n",
       "         317, 172,  14, 210,  61, 436, 214,  66, 359, 543, 216, 176, 286,\n",
       "         474, 513, 549, 181, 399, 584,  23, 220, 112,  73, 330, 225, 405,\n",
       "         113, 447, 184, 478, 260, 188, 145, 484, 593, 294, 147,  75, 410,\n",
       "         152, 451,  78,  79, 454, 415,  40, 416, 267,  44,  83,  45, 528,\n",
       "         564, 343,  48,  88, 122, 424,  90, 565, 124]), array([5, 8, 5, 5, 9, 9, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 9,\n",
       "         5, 5, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 5, 5, 5, 5, 9, 5, 5, 9, 5, 5,\n",
       "         6, 6, 5, 5, 5, 5, 9, 5, 9, 9, 5, 9, 9, 5, 9, 5, 5, 5, 5, 5, 5, 5,\n",
       "         5, 5, 5, 5, 5, 5, 9, 6])))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el = rdd.mapPartitionsWithIndex(func).collect()[0]\n",
    "el"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([22]),)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(el[1][1] == 543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "el[1][2][22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0., 33.,  4.,  8.,  1.,  0.,  0.,  8.,  0.])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54.0, 74)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDocs[0].sum(), len(el[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
