{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neroma Kossi : 3A ENSAE, AS-DS\n",
    "> Projet du cours d'éléments logiciels pour le traitement de données massives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Apache_Spark_logo.svg.png\",width=10,height=10>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Apache_Spark_logo.svg.png\",width=10,height=10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our porject consists of parallelising the Latent Dirichlet Allocation (**LDA**) algorithm. The base paper is [PLDA, a parallel gibbs sampling based algorithm](https://www.semanticscholar.org/paper/PLDA%3A-Parallel-Latent-Dirichlet-Allocation-for-Wang-Bai/376ffb536c3dc5675e9ab875b10b9c4a1437da5d).\n",
    "\n",
    "The main idea is  to run concurrent Gibb's sampling algorithms. This could be done via a distributed framework like MPI or mapReduce, we will be considering the last one in this project. Pyspark will be the standard library for the mapReduce architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    ">## 1. Create the Spark context\n",
    "\n",
    " > ## 2. Data pre-processing\n",
    "  * **2.1. Load the data from file**\n",
    "  * **2.2. Preprocessing**\n",
    "  * **2.3. Building the vocabulary and the set of docs**\n",
    "    * 2.3.1. Building the vocabularies (one per partition)\n",
    "    * 2.3.2. Building docMaps : the set of all the documents (one per partition)\n",
    "    * 2.3.3. Test if vocabularies and docMaps are correctly buil\n",
    "  * **2.4. Prepare the data for the Gibbs samplers**\n",
    "      * 2.4.1. Encode corpus\n",
    "      * 2.4.2. Save the whole work\n",
    "      \n",
    ">## 3. Parallel LDA with mapReduce\n",
    "  * **3.1. Set some parameters**\n",
    "  * **3.2. Run the algorithm**\n",
    "  * **3.3. Post-training analysis**\n",
    "\n",
    ">## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,  SparkContext  # Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math ops\n",
    "import os, shutil, json #File ops\n",
    "import pickle as pkl # Serialiser\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utilities saved into custom modules\n",
    "\n",
    "from nlp import preprocessAndGetTokens\n",
    "from fileUtils import load, pickleLoader, dump, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_memory = '2g' # Max memory available for the driver\n",
    "executor_memory = '400m' # Max memory by executor\n",
    "# We have to set those params before instantiating the SparkContext, other It would be too late\n",
    "pyspark_submit_args = ' --driver-memory {0} --executor-memory {1} pyspark-shell'\\\n",
    "                                .format(driver_memory, executor_memory)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([\n",
    "     ('spark.app.name', 'pLDA'), \n",
    "     ('spark.master', 'local[*]'), # the number of cores is set to max\n",
    "    ('spark.scheduler.mode', 'FAIR')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(conf = conf) # Here we create the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'pLDA'),\n",
       " ('spark.driver.memory', '2g'),\n",
       " ('spark.driver.host', '192.168.0.41'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '37025'),\n",
       " ('spark.app.id', 'local-1549608841145'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data pre-processing\n",
    "\n",
    "\n",
    "> Our dataset is made of **abc-news** article headlines available on [kaggle](https://www.kaggle.com/therohk/million-headlines). the data contains headlines published over a period of 15 years by the reputable Australian news source ABC (Australian Broadcasting Corp.). \n",
    "\n",
    "> Here, we will foucus on  infering interresting topics from this corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load the data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(doc):\n",
    "    \"\"\"This is a wrapper that calls the preprocessAndGetTokens function. The latest function will apply \n",
    "    some basic nlp tehchnics on the paper's abstract : lowercase-isation, stopwords removing, stemming...\"\"\"\n",
    "    \n",
    "    return np.array(preprocessAndGetTokens(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPartitions = 10  # Set the number of partitions, this is important as our Gibbs sampler is designed to \n",
    "                # lunch one sampler per partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Let's read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = spark.pickleFile(\"corpus/bigSample/part-00000\")\n",
    "data = spark.textFile(\"corpus/abc-news/abc-news.csv\")\\\n",
    "            .repartition(nbPartitions)\\\n",
    "                            .map(lambda x : tuple(x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 892 µs, sys: 10.8 ms, total: 11.7 ms\n",
      "Wall time: 2.21 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000090-20030219', 'man jailed over keno fraud'),\n",
       " ('000091-20030219', 'man with knife hijacks light plane'),\n",
       " ('000092-20030219', 'martin to lobby against losing nt seat in fed')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fomart == (doc_id, doc_abstract, doc_title)\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if os.path.exists(\"matrix/docTitles/\") :\n",
    "#     shutil.rmtree(\"matrix/docTitles/\")\n",
    "# data.map(lambda x :  (x[0], x[2])).saveAsPickleFile(\"matrix/docTitles/\") # Save doc titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.41 ms, sys: 4.93 ms, total: 14.3 ms\n",
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Now we do all the preprocessing, and save the dataset\n",
    "folder = \"corpus/train/\"\n",
    "if os.path.exists(folder) :\n",
    "    shutil.rmtree(folder)\n",
    "    \n",
    "data = data.mapValues(processDoc)\\\n",
    "                    .filter(lambda x : len(x[1]) > 0)\\\n",
    "                    \n",
    "data.saveAsPickleFile(\"corpus/train/\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000090-20030219', array(['fraud', 'keno', 'jail', 'man'], dtype='<U5'))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1) # A sample of the tokenized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Here, our dataset is in the primal format `(docId, docTokens)`. Next, we will assign a random topic to each word in a document. We will also need to build the Vocaulary and the set of the documents.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Building the vocabulary and the set of docs (one per partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading and partionning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = spark.pickleFile(\"corpus/train\" ).repartition(nbPartitions)\n",
    "corpus.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 ms, sys: 8.25 ms, total: 9.38 ms\n",
      "Wall time: 2.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000920-20030223',\n",
       "  array(['pope', 'war', 'avert', 'urg', 'blair'], dtype='<U5'))]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Build the vocabularies (one per partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'builder' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/builder.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, builder\n",
    "importlib.reload(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import makeVocabularies, makeVocabulariesFolder, getUniqueWords, getUniqueWords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeVocabulariesFolder() # Instantiate the vocabularies' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13 ms, sys: 14 ms, total: 27 ms\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Here we compute the set of unique words. As word can sometimes be very long, we'd rather retain only their ids\n",
    "# In next steps, we will assign to each word a number ranging from 0 to V-1, where V == size of ours vocabs\n",
    "uniqueWordsByPartition = corpus.mapPartitionsWithIndex(getUniqueWords2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Partition': '00', 'ndocs': 6, 'nvocabs': 4},\n",
       " {'Partition': '01', 'ndocs': 3, 'nvocabs': 3},\n",
       " {'Partition': '02', 'ndocs': 2, 'nvocabs': 4},\n",
       " {'Partition': '03', 'ndocs': 3, 'nvocabs': 7},\n",
       " {'Partition': '04', 'ndocs': 3, 'nvocabs': 3}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents & words per partition\n",
    "\n",
    "L = [{\"Partition\": \"%02d\"%i, \"ndocs\": len(x[0]), \"nvocabs\": len(x[1])} for x, i \n",
    "             in zip(uniqueWordsByPartition, range(nbPartitions))  ]\n",
    "L[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal docs : 30 \n"
     ]
    }
   ],
   "source": [
    "print(\"Totoal docs : %d \"%sum(l[\"ndocs\"] for l in L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary 0 successfully built\n",
      "Vocabulary 1 successfully built\n",
      "Vocabulary 2 successfully built\n",
      "Vocabulary 3 successfully built\n",
      "Vocabulary 4 successfully built\n",
      "Vocabulary 5 successfully built\n",
      "Vocabulary 6 successfully built\n",
      "Vocabulary 7 successfully built\n",
      "Vocabulary 8 successfully built\n",
      "Vocabulary 9 successfully built\n",
      "\n",
      " Global vocabulary  built too\n",
      "CPU times: user 1.24 s, sys: 68.2 ms, total: 1.31 s\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Here we build the vocabularies, one per partition\n",
    "\n",
    "makeVocabularies(uniqueWordsByPartition) # Build and save the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del uniqueWordsByPartition # free up somme memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Make docMaps :  the set of all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import makeDocsMaps, makeDocsMapsFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeDocsMapsFolder() # Instantiate the documents' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.91 ms, sys: 0 ns, total: 7.91 ms\n",
      "Wall time: 819 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['docMap 0 successfully built',\n",
       " 'docMap 1 successfully built',\n",
       " 'docMap 2 successfully built',\n",
       " 'docMap 3 successfully built',\n",
       " 'docMap 4 successfully built',\n",
       " 'docMap 5 successfully built',\n",
       " 'docMap 6 successfully built',\n",
       " 'docMap 7 successfully built',\n",
       " 'docMap 8 successfully built',\n",
       " 'docMap 9 successfully built']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus.mapPartitionsWithIndex(makeDocsMaps).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Test if vocabularies and docMaps are correctly built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As voacabularies & docMaps was successfully built, let's load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 413 ms, sys: 28.3 ms, total: 441 ms\n",
      "Wall time: 459 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabAll = load(\"matrix/vocabulary/vocabAll\")\n",
    "\n",
    "vocabs = [load(\"matrix/vocabulary/vocab__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Vocab :  20715\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in Vocab : \", len(vocabAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 137 ms, sys: 20.5 ms, total: 158 ms\n",
      "Wall time: 159 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from builder import loadDocsAll\n",
    "docsAll = loadDocsAll(nbPartitions)\n",
    "\n",
    "docs = [load(\"matrix/docsMap/docs__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14993, 15008] [8935, 8976]\n",
      "CPU times: user 236 µs, sys: 29 µs, total: 265 µs\n",
      "Wall time: 205 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nbDocs = list(map(len, docs)) # Number of documents per partition\n",
    "nbVocabs = list(map(len, vocabs)) # Number of unique words (vocabulary) per partition\n",
    "print(nbDocs[:2], nbVocabs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Prepare the data for the training step\n",
    "> This step involves encoding the corpus and adding topics : using ids instead of full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Encoding the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import encodeAddTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.82 ms, sys: 0 ns, total: 5.82 ms\n",
      "Wall time: 47.1 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000920-20030223',\n",
       "  array(['pope', 'war', 'avert', 'urg', 'blair'], dtype='<U5'))]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# The corpius is in full text again, let's change it in the next step\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTopics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can notice that all the words have been encoded into symbolic ids, topics  have been added too\n",
    "corpus2 = corpus.mapPartitionsWithIndex(lambda ind, part : encodeAddTopics(ind, part,docs[ind],\n",
    "                                                                           vocabs[ind], nbTopics), \n",
    "                                       preservesPartitioning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.34 s, sys: 226 ms, total: 8.56 s\n",
      "Wall time: 9.23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, (6724, array([6001, 8581,  522, 8373,  866]), array([2, 3, 1, 5, 3])))]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus2.take(1) # Just word's and doc's ids now, topics have been added too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Save the whole work for the next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fileUtils' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/fileUtils.py'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fileUtils\n",
    "importlib.reload(fileUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileUtils import saveAsPickleFile, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.35 s, sys: 59.6 ms, total: 8.41 s\n",
      "Wall time: 17.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saveAsPickleFile(corpus2)\n",
    "if os.path.exists(\"initial_train\"):\n",
    "    shutil.rmtree(\"initial_train\")\n",
    "corpus2.saveAsPickleFile(\"initial_train\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# corpus2.mapPartitionsWithIndex(lambda ind, part :\n",
    "#                     saveByPartition(ind, part, \"corpus/train2\", batchsize=10))\\\n",
    "#                             .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is the end of the data preprocessing, the data is in the right format now and we can run our `Gibbs samplers`. Let's sart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, corpus, corpus2 # free up some memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parallel LDA (mapReduce version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbVocabAll = len(vocabAll)\n",
    "alpha = 0.5\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builder import makeConfig, updateConfig, get_now\n",
    "# makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, model, builder, fileUtils\n",
    "importlib.reload(model)\n",
    "importlib.reload(builder)\n",
    "importlib.reload(fileUtils)\n",
    "from fileUtils import saveAsPickleFile\n",
    "from model import pldaMap0\n",
    "from builder import updateCountWordsAll, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pldaMap(0, 1, alpha, beta, len(vocabAll), nbTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = spark.pickleFile(\"pickle/\")\n",
    "# rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (14685, array([8740, 1817, 3997, 1154]), array([2, 3, 1, 5])))]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.pickleFile(\"initial_train\")\n",
    "# (doc_id, doc_words, doc_topics) <--- the format\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, Elapsed : 35.27850651741028\n",
      "iteration : 10, Elapsed : 339.45840883255005\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "rdd = spark.pickleFile(\"initial_train\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "init(rdd, vocabs, nbDocs, nbVocabs, len(vocabAll), nbTopics)\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    rdd = rdd.mapPartitionsWithIndex(lambda ind, part : pldaMap0(ind, part, alpha, beta, nbVocabAll, nbTopics),\n",
    "                       preservesPartitioning= True )\n",
    "    saveAsPickleFile(rdd)\n",
    "    rdd = spark.pickleFile(\"pickle/\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "    updateCountWordsAll()\n",
    "    if i%10 == 0 :\n",
    "        print(\"iteration : {0}, Elapsed : {1}\".format(i, time.time() - t0))\n",
    "print(\"Total time : {}\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(list(vocabAll.items())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords = countWords/countWords.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntop = 15\n",
    "order = np.argsort(countWords, 0)[::-1]\n",
    "\n",
    "topics = pd.DataFrame(words[order[:ntop], 0])\n",
    "topics.columns = [\"topic%i\"%i for i  in topics.columns]\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we can see, the model successfully finds meaningful topics in less than hundred gibbs sampling steps. The `topic0` seems to talk about **car crashes**, the `topic9` is clearly about **Iraq** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This project is very educational. Not only did it allow us to deepen our knowledge of Spark, but also to implement one of the most widely used topic modeling algorithms.\n",
    "\n",
    ">The introduction of parallel Gibbs sampling by Wang & Al. helps to bypass the sequential nature of MCMC algorithms and to take advantage of the power of tools such as Spark.\n",
    "\n",
    ">A next step would have been to be able to analyze the speed-up gained via this parallelized scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
