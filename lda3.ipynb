{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, shutil, json, time\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from nlp import preprocessAndGetTokens\n",
    "# from paperAbstracts import processPaperAbstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileUtils import load, pickleLoader, dump, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"lda\") \\\n",
    "    .getOrCreate().sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processPaperAbstract(abstract):\n",
    "    return np.array(preprocessAndGetTokens(abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadPaperAbstract(docstr):\n",
    "    doc = json.loads(docstr)\n",
    "    return (doc[\"id\"], doc[\"paperAbstract\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.textFile(\"sample-S2-records\").map(LoadPaperAbstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e',\n",
       "  'Primary debulking surgery (PDS) has historically been the standard treatment for advanced ovarian cancer. Recent data appear to support a paradigm shift toward neoadjuvant chemotherapy with interval debulking surgery (NACT-IDS). We hypothesized that stage IV ovarian cancer patients would likely benefit from NACT-IDS by achieving similar outcomes with less morbidity. Patients with stage IV epithelial ovarian cancer who underwent primary treatment between January 1, 1995 and December 31, 2007, were identified. Data were retrospectively extracted. Each patient record was evaluated to subclassify stage IV disease according to the sites of tumor dissemination at the time of diagnosis. The Kaplanâ€“Meier method was used to compare overall survival (OS) data. A total of 242 newly diagnosed stage IV epithelial ovarian cancer patients were included in the final analysis; 176 women (73%) underwent PDS, 45 (18%) NACT-IDS, and 21 (9%) chemotherapy only. The frequency of achieving complete resection to no residual disease was significantly higher in patients with NACT-IDS versus PDS (27% vs. 7.5%; P\\xa0<\\xa00.001). When compared to women treated with NACT-IDS, women with PDS had longer admissions (12 vs. 8\\xa0days; P\\xa0=\\xa00.01), more frequent intensive care unit admissions (12% vs. 0%; P\\xa0=\\xa00.01), and a trend toward a higher rate of postoperative complications (27% vs. 15%; P\\xa0=\\xa00.08). The patients who received only chemotherapy had a median OS of 23\\xa0months, compared to 33\\xa0months in the NACT-IDS group and 29\\xa0months in the PDS group (P\\xa0=\\xa00.1). NACT-IDS for stage IV ovarian cancer resulted in higher rates of complete resection to no residual disease, less morbidity, and equivalent OS compared to PDS.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e',\n",
       "  array(['unit', 'analysi', 'compar', 'includ', 'histor', 'decemb',\n",
       "         'surgeri', 'subclassifi', 'paradigm', 'chemotherapi', 'treatment',\n",
       "         'diagnosi', 'cancer', 'trend', 'complet', 'januari', 'appear',\n",
       "         'admiss', 'recent', 'final', 'toward', 'complic', 'result',\n",
       "         'accord', 'receiv', 'resect', 'ovarian', 'frequenc', 'os',\n",
       "         'support', 'treat', 'method', 'outcom', 'use', 'versus', 'patient',\n",
       "         'postop', 'extract', 'evalu', 'time', 'standard', 'diagnos',\n",
       "         'tumor', 'nactid', 'retrospect', 'achiev', 'overal', 'higher',\n",
       "         'kaplanmei', 'vs', 'record', 'advanc', 'diseas', 'iv', 'pds',\n",
       "         'shift', 'site', 'stage', 'identifi', 'neoadjuv', 'intens', 'like',\n",
       "         'residu', 'epitheli', 'less', 'benefit', 'hypothes', 'total',\n",
       "         'longer', 'group', 'underw', 'month', 'surviv', 'median',\n",
       "         'dissemin', 'data', 'signific', 'interv', 'morbid', 'similar',\n",
       "         'newli', 'frequent', 'would', 'women', 'rate', 'care', 'primari',\n",
       "         'day', 'debulk', 'equival'], dtype='<U12'))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data.mapValues(processPaperAbstract).filter(lambda x : len(x[1]) > 0)\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.map(lambda x: len(x[1])).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patitionning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPartitions = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomPartitionner(x, nbPartitions):\n",
    "    return (np.random.choice(nbPartitions),x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = corpus.map(lambda x  : randomPartitionner(x, nbPartitions))\\\n",
    "            .partitionBy(nbPartitions).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ('cb61fc1ebdeb5835460c18044d331388d5b1067a',\n",
       "   array(['allopurinol', 'peripher', 'immedi', 'control', 'blood', 'group',\n",
       "          'anaesthet', 'rat', 'oxygenfre', 'dismutas', 'cardiovascular',\n",
       "          'resist', 'involv', 'prevent', 'rise', 'improv', 'free', 'scald',\n",
       "          'burn', 'scaveng', 'catalas', 'signific', 'fall', 'blocker',\n",
       "          'pressur', 'cent', 'product', 'per', 'oxygen', 'increas',\n",
       "          'investig', 'rate', 'surfac', 'plus', 'treatment', 'cardiac',\n",
       "          'mean', 'appli', 'seen', 'arteri', 'caus', 'output', 'superoxid',\n",
       "          'slight', 'infus', 'pretreat', 'radic', 'affect', 'total', 'area',\n",
       "          'suggest', 'howev', 'heart', 'injuri', 'bodi', 'lower'],\n",
       "         dtype='<U14')))]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import makeVocabularies, getUniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words by partition : [645, 446, 556, 540, 571, 397, 310, 218, 480, 324]\n"
     ]
    }
   ],
   "source": [
    "uniqueWordsByPartition = corpus2.mapPartitionsWithIndex(getUniqueWords).collect()\n",
    "\n",
    "print( \"Number of unique words by partition : %s\"%str([len(u) for u in uniqueWordsByPartition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary 0 successfully built\n",
      "Vocabulary 1 successfully built\n",
      "Vocabulary 2 successfully built\n",
      "Vocabulary 3 successfully built\n",
      "Vocabulary 4 successfully built\n",
      "Vocabulary 5 successfully built\n",
      "Vocabulary 6 successfully built\n",
      "Vocabulary 7 successfully built\n",
      "Vocabulary 8 successfully built\n",
      "Vocabulary 9 successfully built\n",
      "\n",
      " Global vocabulary  built too\n"
     ]
    }
   ],
   "source": [
    "makeVocabularies(uniqueWordsByPartition) # Build and save the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make docMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import makeDocsMaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docMap 0 successfully built',\n",
       " 'docMap 1 successfully built',\n",
       " 'docMap 2 successfully built',\n",
       " 'docMap 3 successfully built',\n",
       " 'docMap 4 successfully built',\n",
       " 'docMap 5 successfully built',\n",
       " 'docMap 6 successfully built',\n",
       " 'docMap 7 successfully built',\n",
       " 'docMap 8 successfully built',\n",
       " 'docMap 9 successfully built']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2.mapPartitionsWithIndex(makeDocsMaps).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test if vocabularies and docMaps are correctly built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabSize : 2701 \n"
     ]
    }
   ],
   "source": [
    "vocabSize = len(load(\"matrix/vocabulary/vocabAll\"))\n",
    "print(\"vocabSize : %d \"%vocabSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs in docs00 : 10\n"
     ]
    }
   ],
   "source": [
    "ndocs00 = len(load(\"matrix/docsMap/docs__0000__\"))\n",
    "print(\"Number of docs in docs00 : %d\"%ndocs00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# As voacabularies & docMaps was well built, let's load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabAll = load(\"matrix/vocabulary/vocabAll\")\n",
    "\n",
    "vocabs = [load(\"matrix/vocabulary/vocab__%04d__\"%ind) for ind in range(nbPartitions)] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import loadDocsAll\n",
    "docsAll = loadDocsAll(nbPartitions)\n",
    "\n",
    "docs = [load(\"matrix/docsMap/docs__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbDocs = list(map(len, docs))\n",
    "nbVocabs = list(map(len, vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode corpus : using ids instead of doc full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  ('cb61fc1ebdeb5835460c18044d331388d5b1067a',\n",
       "   array(['allopurinol', 'peripher', 'immedi', 'control', 'blood', 'group',\n",
       "          'anaesthet', 'rat', 'oxygenfre', 'dismutas', 'cardiovascular',\n",
       "          'resist', 'involv', 'prevent', 'rise', 'improv', 'free', 'scald',\n",
       "          'burn', 'scaveng', 'catalas', 'signific', 'fall', 'blocker',\n",
       "          'pressur', 'cent', 'product', 'per', 'oxygen', 'increas',\n",
       "          'investig', 'rate', 'surfac', 'plus', 'treatment', 'cardiac',\n",
       "          'mean', 'appli', 'seen', 'arteri', 'caus', 'output', 'superoxid',\n",
       "          'slight', 'infus', 'pretreat', 'radic', 'affect', 'total', 'area',\n",
       "          'suggest', 'howev', 'heart', 'injuri', 'bodi', 'lower'],\n",
       "         dtype='<U14')))]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that all the words have been encoded into symbolic ids\n",
    "corpus2 = corpus2.mapPartitionsWithIndex(lambda ind, part : encode(ind, part, docs, vocabs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End data preparation by adding random topics and saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTopics = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpTop = corpus2.map(lambda x : (x[0], x[1], np.random.choice(nbTopics, len(x[1]) ))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.mapPartitionsWithIndex(saveByPartition).collect() # Save bacthes by partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 26, 419, 289, 136,  75, 269,  30, 475, 406, 180,  89, 497, 313,\n",
       "         448, 507, 293, 250, 513,  80, 516,  90, 540, 236,  74, 446,  98,\n",
       "         455, 416, 405, 295, 312, 476, 580, 427, 611,  88, 349,  40, 523,\n",
       "          46,  92, 402, 577, 550, 300, 447, 471,  21, 604,  43, 573, 283,\n",
       "         275, 304,  76, 338]), array([2, 3, 4, 1, 4, 3, 2, 4, 0, 0, 1, 4, 3, 1, 1, 2, 4, 4, 2, 4, 3, 4,\n",
       "         3, 2, 0, 1, 0, 2, 3, 3, 4, 0, 4, 0, 1, 0, 3, 3, 3, 3, 0, 2, 3, 2,\n",
       "         1, 1, 0, 1, 0, 1, 0, 2, 4, 4, 2, 0]))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fileUtils' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/fileUtils.py'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fileUtils, importlib\n",
    "importlib.reload(fileUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here the ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbVocabAll = len(vocabAll)\n",
    "alpha = 100\n",
    "beta = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builder import makeConfig, updateConfig, get_now\n",
    "# makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileUtils import saveAsPickleFile\n",
    "from model import pldaMap0\n",
    "from builder import updateCountWordsAll, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pldaMap(0, 1, alpha, beta, len(vocabAll), nbTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time : 43.2295606136322\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "rdd = corpTop\n",
    "init(corpTop, vocabs, nbDocs, nbVocabs, len(vocabAll), nbTopics)\n",
    "for i in range(500):\n",
    "    rdd = rdd.mapPartitionsWithIndex(lambda ind, part : pldaMap0(ind, part, alpha, beta, nbVocabAll, nbTopics),\n",
    "                       preservesPartitioning= True )\n",
    "    saveAsPickleFile(rdd)\n",
    "    rdd = spark.pickleFile(\"pickle/\")\n",
    "    updateCountWordsAll()\n",
    "print(\"Time : {}\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.,  7.,  8.,  9., 10.],\n",
       "       [13., 17., 28., 24., 22.],\n",
       "       [11., 18., 24., 16., 14.],\n",
       "       [22., 19., 18., 24., 17.],\n",
       "       [25., 24., 27., 32., 19.],\n",
       "       [13., 13.,  9., 20., 15.],\n",
       "       [15., 16., 18., 13., 15.],\n",
       "       [13., 11., 12.,  7., 11.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = 0\n",
    "subdoc = 2\n",
    "countDocs = load(\"matrix/countDocs/docs__%04d__\"%subdoc)\n",
    "countDocs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13., 11., 12.,  7., 11.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics = countDocs.argmax(1)\n",
    "v = np.where(topics == cl)\n",
    "countDocs[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['248647e053c227b6375d119bb16a7508fdfbef2b', '7']], dtype='<U40')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dks = np.array( list(docs[subdoc].items()))\n",
    "cluster = dks[v]\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('248647e053c227b6375d119bb16a7508fdfbef2b',\n",
       "  array(['result', 'sea', 'theori', 'wave', 'eastern', 'coast', 'discuss',\n",
       "         'critic', 'zone', 'indic', 'compar', 'linear', 'propag', 'differ',\n",
       "         'notic', 'model', 'accuraci', 'shallow', 'employ', 'practic',\n",
       "         'tsunami', 'south', 'equat', 'neglect', 'depth', 'appropri',\n",
       "         'nonlinear', 'seismogen', 'would', 'term', 'furthermor', 'verifi',\n",
       "         'bottom', 'china', 'also', 'appli', 'chines', 'describ', 'applic',\n",
       "         'forecast', 'influenc', 'region', 'exert', 'characterist', 'ocean',\n",
       "         'simul', 'paper', 'friction', 'enough', 'water', 'hydrodynam',\n",
       "         'along', 'potenti', 'base'], dtype='<U12'))]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.filter(lambda x : np.isin(x[0], cluster[:, 0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
