{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neroma Kossi : 3A ENSAE, AS-DS\n",
    "> Projet du cours d'éléments logiciels pour le traitement de données massives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Apache_Spark_logo.svg.png\",width=10,height=10>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<img src=\"Apache_Spark_logo.svg.png\",width=10,height=10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our porject consists of parallelising the Latent Dirichlet Allocation (**LDA**) algorithm. The base paper is [PLDA, a parallel gibbs sampling based algorithm](https://www.semanticscholar.org/paper/PLDA%3A-Parallel-Latent-Dirichlet-Allocation-for-Wang-Bai/376ffb536c3dc5675e9ab875b10b9c4a1437da5d).\n",
    "\n",
    "The main idea is  to run concurrent Gibb's sampling algorithms. This could be done via a distributed framework like MPI or mapReduce, we will be considering the last one in this project. Pyspark will be the standard library for the mapReduce architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    ">## 1. Create the Spark context\n",
    "\n",
    " > ## 2. Data pre-processing\n",
    "  * **2.1. Load the data from file**\n",
    "  * **2.2. Preprocessing**\n",
    "  * **2.3. Building the vocabulary and the set of docs**\n",
    "    * 2.3.1. Building the vocabularies (one per partition)\n",
    "    * 2.3.2. Building docMaps : the set of all the documents (one per partition)\n",
    "    * 2.3.3. Test if vocabularies and docMaps are correctly buil\n",
    "  * **2.4. Prepare the data for the Gibbs samplers**\n",
    "      * 2.4.1. Encode corpus\n",
    "      * 2.4.2. Save the whole work\n",
    "      \n",
    ">## 3. Parallel LDA with mapReduce\n",
    "  * **3.1. Set some parameters**\n",
    "  * **3.2. Run the algorithm**\n",
    "  * **3.3. Post-training analysis**\n",
    "\n",
    ">## 4. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,  SparkContext  # Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # math ops\n",
    "import os, shutil, json #File ops\n",
    "import pickle as pkl # Serialiser\n",
    "\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utilities saved into custom modules\n",
    "\n",
    "from nlp import preprocessAndGetTokens\n",
    "from fileUtils import load, pickleLoader, dump, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the Spark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver_memory = '1g' # Max memory available for the driver\n",
    "executor_memory = '200m' # Max memory by executor\n",
    "# We have to set those params before instantiating the SparkContext, other It would be too late\n",
    "pyspark_submit_args = ' --driver-memory {0} --executor-memory {1} pyspark-shell'\\\n",
    "                                .format(driver_memory, executor_memory)\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAll([\n",
    "     ('spark.app.name', 'pLDA'), \n",
    "     ('spark.master', 'local[*]'), # the number of cores is set to max\n",
    "    ('spark.scheduler.mode', 'FAIR')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkContext(conf = conf) # Here we create the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1549604916154'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.name', 'pLDA'),\n",
       " ('spark.scheduler.mode', 'FAIR'),\n",
       " ('spark.driver.host', '192.168.0.41'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.memory', '1g'),\n",
       " ('spark.driver.port', '40305'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data pre-processing\n",
    "\n",
    "\n",
    "> Our dataset is made of **abc-news** article headlines available on [kaggle](https://www.kaggle.com/therohk/million-headlines). Here, we will try to infer interesting topics from this corpus ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Load the data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDoc(doc):\n",
    "    \"\"\"This is a wrapper that calls the preprocessAndGetTokens function. The latest function will apply \n",
    "    some basic nlp tehchnics on the paper's abstract : lowercase-isation, stopwords removing, stemming...\"\"\"\n",
    "    \n",
    "    return np.array(preprocessAndGetTokens(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPartitions = 10  # Set the number of partitions, this is important as our Gibbs sampler is designed to \n",
    "                # lunch one sampler per partition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Let's read the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = spark.pickleFile(\"corpus/bigSample/part-00000\")\n",
    "data = spark.textFile(\"corpus/abc-news/abc-news.csv\")\\\n",
    "            .repartition(nbPartitions)\\\n",
    "                            .map(lambda x : tuple(x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 ms, sys: 169 µs, total: 10.4 ms\n",
      "Wall time: 1.69 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000090-20030219', 'man jailed over keno fraud'),\n",
       " ('000091-20030219', 'man with knife hijacks light plane'),\n",
       " ('000092-20030219', 'martin to lobby against losing nt seat in fed')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# fomart == (doc_id, doc_abstract, doc_title)\n",
    "data.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# if os.path.exists(\"matrix/docTitles/\") :\n",
    "#     shutil.rmtree(\"matrix/docTitles/\")\n",
    "# data.map(lambda x :  (x[0], x[2])).saveAsPickleFile(\"matrix/docTitles/\") # Save doc titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.4 ms, sys: 40.3 ms, total: 54.7 ms\n",
      "Wall time: 5.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Now we do all the preprocessing, and save the dataset\n",
    "folder = \"corpus/train/\"\n",
    "if os.path.exists(folder) :\n",
    "    shutil.rmtree(folder)\n",
    "    \n",
    "data = data.mapValues(processDoc)\\\n",
    "                    .filter(lambda x : len(x[1]) > 0)\\\n",
    "                    \n",
    "data.saveAsPickleFile(\"corpus/train/\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('000090-20030219', array(['fraud', 'keno', 'jail', 'man'], dtype='<U5'))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(1) # A sample of the tokenized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***Here, our dataset is in the primal format `(docId, docTokens)`. Next, we will assign a random topic to each word in a document. We will also need to build the Vocaulary and the set of the documents.*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Building the vocabulary and the set of docs (one per partition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reloading and partionning the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = spark.pickleFile(\"corpus/train\" ).repartition(nbPartitions)\n",
    "corpus.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.79 ms, sys: 3.47 ms, total: 10.3 ms\n",
      "Wall time: 1.11 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000920-20030223',\n",
       "  array(['pope', 'war', 'avert', 'urg', 'blair'], dtype='<U5'))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1. Build the vocabularies (one per partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'builder' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/builder.py'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib, builder\n",
    "importlib.reload(builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import makeVocabularies, makeVocabulariesFolder, getUniqueWords, getUniqueWords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeVocabulariesFolder() # Instantiate the vocabularies' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.5 ms, sys: 4.27 ms, total: 18.8 ms\n",
      "Wall time: 782 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Here we compute the set of unique words. As word can sometimes be very long, we'd rather retain only their ids\n",
    "# In next steps, we will assign to each word a number ranging from 0 to V-1, where V == size of ours vocabs\n",
    "uniqueWordsByPartition = corpus.mapPartitionsWithIndex(getUniqueWords2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Partition': '00', 'ndocs': 6, 'nvocabs': 3},\n",
       " {'Partition': '01', 'ndocs': 3, 'nvocabs': 4},\n",
       " {'Partition': '02', 'ndocs': 3, 'nvocabs': 6},\n",
       " {'Partition': '03', 'ndocs': 6, 'nvocabs': 2},\n",
       " {'Partition': '04', 'ndocs': 3, 'nvocabs': 7}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of documents & words per partition\n",
    "\n",
    "L = [{\"Partition\": \"%02d\"%i, \"ndocs\": len(x[0]), \"nvocabs\": len(x[1])} for x, i \n",
    "             in zip(uniqueWordsByPartition, range(nbPartitions))  ]\n",
    "L[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totoal docs : 35 \n"
     ]
    }
   ],
   "source": [
    "print(\"Totoal docs : %d \"%sum(l[\"ndocs\"] for l in L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary 0 successfully built\n",
      "Vocabulary 1 successfully built\n",
      "Vocabulary 2 successfully built\n",
      "Vocabulary 3 successfully built\n",
      "Vocabulary 4 successfully built\n",
      "Vocabulary 5 successfully built\n",
      "Vocabulary 6 successfully built\n",
      "Vocabulary 7 successfully built\n",
      "Vocabulary 8 successfully built\n",
      "Vocabulary 9 successfully built\n",
      "\n",
      " Global vocabulary  built too\n",
      "CPU times: user 869 ms, sys: 36.2 ms, total: 906 ms\n",
      "Wall time: 851 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Here we build the vocabularies, one per partition\n",
    "\n",
    "makeVocabularies(uniqueWordsByPartition) # Build and save the vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "del uniqueWordsByPartition # free up somme memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2. Make docMaps :  the set of all the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder import makeDocsMaps, makeDocsMapsFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeDocsMapsFolder() # Instantiate the documents' folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.88 ms, sys: 4.33 ms, total: 8.21 ms\n",
      "Wall time: 452 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['docMap 0 successfully built',\n",
       " 'docMap 1 successfully built',\n",
       " 'docMap 2 successfully built',\n",
       " 'docMap 3 successfully built',\n",
       " 'docMap 4 successfully built',\n",
       " 'docMap 5 successfully built',\n",
       " 'docMap 6 successfully built',\n",
       " 'docMap 7 successfully built',\n",
       " 'docMap 8 successfully built',\n",
       " 'docMap 9 successfully built']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "corpus.mapPartitionsWithIndex(makeDocsMaps).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3. Test if vocabularies and docMaps are correctly built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As voacabularies & docMaps was successfully built, let's load them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 181 ms, sys: 15.3 ms, total: 196 ms\n",
      "Wall time: 200 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabAll = load(\"matrix/vocabulary/vocabAll\")\n",
    "\n",
    "vocabs = [load(\"matrix/vocabulary/vocab__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in Vocab :  13998\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words in Vocab : \", len(vocabAll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.8 ms, sys: 8.11 ms, total: 37.9 ms\n",
      "Wall time: 37.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from builder import loadDocsAll\n",
    "docsAll = loadDocsAll(nbPartitions)\n",
    "\n",
    "docs = [load(\"matrix/docsMap/docs__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4990, 5010] [5470, 5395]\n",
      "CPU times: user 201 µs, sys: 19 µs, total: 220 µs\n",
      "Wall time: 149 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "nbDocs = list(map(len, docs)) # Number of documents per partition\n",
    "nbVocabs = list(map(len, vocabs)) # Number of unique words (vocabulary) per partition\n",
    "print(nbDocs[:2], nbVocabs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Prepare the data for the training step\n",
    "> This step involves encoding the corpus and adding topics : using ids instead of full text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.1. Encoding the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import encodeAddTopics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.33 ms, sys: 331 µs, total: 7.66 ms\n",
      "Wall time: 47.3 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('000920-20030223',\n",
       "  array(['pope', 'war', 'avert', 'urg', 'blair'], dtype='<U5'))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "# The corpius is in full text again, let's change it in the next step\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTopics = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can notice that all the words have been encoded into symbolic ids, topics  have been added too\n",
    "corpus2 = corpus.mapPartitionsWithIndex(lambda ind, part : encodeAddTopics(ind, part,docs[ind],\n",
    "                                                                           vocabs[ind], nbTopics), \n",
    "                                       preservesPartitioning = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.19 s, sys: 106 ms, total: 4.3 s\n",
      "Wall time: 4.63 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, (2182, array([3668, 5275,  338, 5162,  526]), array([8, 6, 0, 0, 2])))]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "corpus2.take(1) # Just word's and doc's ids now, topics have been added too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2. Save the whole work for the next step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'fileUtils' from '/home/nerk/Documents/3A_ENSAE/mapReduceLda/fileUtils.py'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fileUtils\n",
    "importlib.reload(fileUtils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fileUtils import saveAsPickleFile, saveByPartition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 s, sys: 8.96 ms, total: 4.17 s\n",
      "Wall time: 7.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# saveAsPickleFile(corpus2)\n",
    "if os.path.exists(\"initial_train\"):\n",
    "    shutil.rmtree(\"initial_train\")\n",
    "corpus2.saveAsPickleFile(\"initial_train\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# corpus2.mapPartitionsWithIndex(lambda ind, part :\n",
    "#                     saveByPartition(ind, part, \"corpus/train2\", batchsize=10))\\\n",
    "#                             .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here is the end of the data preprocessing, the data is in the right format now and we can run our `Gibbs samplers`. Let's sart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, corpus, corpus2 # free up some memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Parallel LDA (mapReduce version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Here the ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbVocabAll = len(vocabAll)\n",
    "alpha = 0.5\n",
    "beta = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from builder  import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from builder import makeConfig, updateConfig, get_now\n",
    "# makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, model, builder, fileUtils\n",
    "importlib.reload(model)\n",
    "importlib.reload(builder)\n",
    "importlib.reload(fileUtils)\n",
    "from fileUtils import saveAsPickleFile\n",
    "from model import pldaMap0\n",
    "from builder import updateCountWordsAll, init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pldaMap(0, 1, alpha, beta, len(vocabAll), nbTopics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = spark.pickleFile(\"pickle/\")\n",
    "# rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, (4896, array([5344, 1079, 2409,  676]), array([8, 6, 0, 0])))]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.pickleFile(\"initial_train\")\n",
    "# (doc_id, doc_words, doc_topics) <--- the format\n",
    "rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0, Elapsed : 13.784972429275513\n",
      "iteration : 10, Elapsed : 137.8090603351593\n",
      "iteration : 20, Elapsed : 258.8914649486542\n",
      "iteration : 30, Elapsed : 386.84273862838745\n",
      "iteration : 40, Elapsed : 499.95032572746277\n",
      "Total time : 594.2944474220276\n",
      "CPU times: user 11.6 s, sys: 849 ms, total: 12.5 s\n",
      "Wall time: 9min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "t0 = time.time()\n",
    "rdd = spark.pickleFile(\"initial_train\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "init(rdd, vocabs, nbDocs, nbVocabs, len(vocabAll), nbTopics)\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    rdd = rdd.mapPartitionsWithIndex(lambda ind, part : pldaMap0(ind, part, alpha, beta, nbVocabAll, nbTopics),\n",
    "                       preservesPartitioning= True )\n",
    "    saveAsPickleFile(rdd)\n",
    "    rdd = spark.pickleFile(\"pickle/\").partitionBy(nbPartitions).map(lambda x: x[1])\n",
    "    updateCountWordsAll()\n",
    "    if i%10 == 0 :\n",
    "        print(\"iteration : {0}, Elapsed : {1}\".format(i, time.time() - t0))\n",
    "print(\"Total time : {}\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Post-training analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = np.array(list(vocabAll.items())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords = countWords/countWords.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>polic</td>\n",
       "      <td>say</td>\n",
       "      <td>plan</td>\n",
       "      <td>man</td>\n",
       "      <td>win</td>\n",
       "      <td>new</td>\n",
       "      <td>water</td>\n",
       "      <td>us</td>\n",
       "      <td>govt</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fire</td>\n",
       "      <td>warn</td>\n",
       "      <td>fund</td>\n",
       "      <td>charg</td>\n",
       "      <td>world</td>\n",
       "      <td>rise</td>\n",
       "      <td>concern</td>\n",
       "      <td>talk</td>\n",
       "      <td>urg</td>\n",
       "      <td>kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>crash</td>\n",
       "      <td>home</td>\n",
       "      <td>council</td>\n",
       "      <td>court</td>\n",
       "      <td>cup</td>\n",
       "      <td>rate</td>\n",
       "      <td>air</td>\n",
       "      <td>south</td>\n",
       "      <td>claim</td>\n",
       "      <td>iraq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probe</td>\n",
       "      <td>sar</td>\n",
       "      <td>get</td>\n",
       "      <td>face</td>\n",
       "      <td>final</td>\n",
       "      <td>high</td>\n",
       "      <td>plan</td>\n",
       "      <td>iraq</td>\n",
       "      <td>call</td>\n",
       "      <td>iraqi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>continu</td>\n",
       "      <td>war</td>\n",
       "      <td>job</td>\n",
       "      <td>polic</td>\n",
       "      <td>lead</td>\n",
       "      <td>price</td>\n",
       "      <td>group</td>\n",
       "      <td>korea</td>\n",
       "      <td>nsw</td>\n",
       "      <td>attack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>car</td>\n",
       "      <td>iraq</td>\n",
       "      <td>drought</td>\n",
       "      <td>murder</td>\n",
       "      <td>open</td>\n",
       "      <td>show</td>\n",
       "      <td>call</td>\n",
       "      <td>un</td>\n",
       "      <td>council</td>\n",
       "      <td>bomb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>death</td>\n",
       "      <td>return</td>\n",
       "      <td>farmer</td>\n",
       "      <td>miss</td>\n",
       "      <td>top</td>\n",
       "      <td>ban</td>\n",
       "      <td>council</td>\n",
       "      <td>set</td>\n",
       "      <td>reject</td>\n",
       "      <td>report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>road</td>\n",
       "      <td>may</td>\n",
       "      <td>worker</td>\n",
       "      <td>trial</td>\n",
       "      <td>take</td>\n",
       "      <td>record</td>\n",
       "      <td>servic</td>\n",
       "      <td>coast</td>\n",
       "      <td>vic</td>\n",
       "      <td>baghdad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>investig</td>\n",
       "      <td>australian</td>\n",
       "      <td>mine</td>\n",
       "      <td>drug</td>\n",
       "      <td>titl</td>\n",
       "      <td>year</td>\n",
       "      <td>health</td>\n",
       "      <td>north</td>\n",
       "      <td>plan</td>\n",
       "      <td>dead</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>die</td>\n",
       "      <td>howard</td>\n",
       "      <td>new</td>\n",
       "      <td>jail</td>\n",
       "      <td>england</td>\n",
       "      <td>fall</td>\n",
       "      <td>doctor</td>\n",
       "      <td>pm</td>\n",
       "      <td>pay</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>begin</td>\n",
       "      <td>back</td>\n",
       "      <td>boost</td>\n",
       "      <td>appeal</td>\n",
       "      <td>tour</td>\n",
       "      <td>put</td>\n",
       "      <td>fear</td>\n",
       "      <td>trade</td>\n",
       "      <td>support</td>\n",
       "      <td>protest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>driver</td>\n",
       "      <td>bush</td>\n",
       "      <td>seek</td>\n",
       "      <td>case</td>\n",
       "      <td>set</td>\n",
       "      <td>big</td>\n",
       "      <td>highlight</td>\n",
       "      <td>china</td>\n",
       "      <td>wa</td>\n",
       "      <td>blast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>hospit</td>\n",
       "      <td>australia</td>\n",
       "      <td>govt</td>\n",
       "      <td>search</td>\n",
       "      <td>clash</td>\n",
       "      <td>back</td>\n",
       "      <td>new</td>\n",
       "      <td>east</td>\n",
       "      <td>act</td>\n",
       "      <td>troop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>nsw</td>\n",
       "      <td>solomon</td>\n",
       "      <td>industri</td>\n",
       "      <td>death</td>\n",
       "      <td>back</td>\n",
       "      <td>boost</td>\n",
       "      <td>resid</td>\n",
       "      <td>deal</td>\n",
       "      <td>minist</td>\n",
       "      <td>war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>accid</td>\n",
       "      <td>pm</td>\n",
       "      <td>help</td>\n",
       "      <td>hear</td>\n",
       "      <td>aussi</td>\n",
       "      <td>expect</td>\n",
       "      <td>restrict</td>\n",
       "      <td>new</td>\n",
       "      <td>defend</td>\n",
       "      <td>soldier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic0      topic1    topic2  topic3   topic4  topic5     topic6 topic7  \\\n",
       "0      polic         say      plan     man      win     new      water     us   \n",
       "1       fire        warn      fund   charg    world    rise    concern   talk   \n",
       "2      crash        home   council   court      cup    rate        air  south   \n",
       "3      probe         sar       get    face    final    high       plan   iraq   \n",
       "4    continu         war       job   polic     lead   price      group  korea   \n",
       "5        car        iraq   drought  murder     open    show       call     un   \n",
       "6      death      return    farmer    miss      top     ban    council    set   \n",
       "7       road         may    worker   trial     take  record     servic  coast   \n",
       "8   investig  australian      mine    drug     titl    year     health  north   \n",
       "9        die      howard       new    jail  england    fall     doctor     pm   \n",
       "10     begin        back     boost  appeal     tour     put       fear  trade   \n",
       "11    driver        bush      seek    case      set     big  highlight  china   \n",
       "12    hospit   australia      govt  search    clash    back        new   east   \n",
       "13       nsw     solomon  industri   death     back   boost      resid   deal   \n",
       "14     accid          pm      help    hear    aussi  expect   restrict    new   \n",
       "\n",
       "     topic8   topic9  \n",
       "0      govt       us  \n",
       "1       urg     kill  \n",
       "2     claim     iraq  \n",
       "3      call    iraqi  \n",
       "4       nsw   attack  \n",
       "5   council     bomb  \n",
       "6    reject   report  \n",
       "7       vic  baghdad  \n",
       "8      plan     dead  \n",
       "9       pay      two  \n",
       "10  support  protest  \n",
       "11       wa    blast  \n",
       "12      act    troop  \n",
       "13   minist      war  \n",
       "14   defend  soldier  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ntop = 15\n",
    "order = np.argsort(countWords, 0)[::-1]\n",
    "\n",
    "topics = pd.DataFrame(words[order[:ntop], 0])\n",
    "topics.columns = [\"topic%i\"%i for i  in topics.columns]\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As we can see, the model successfully finds meaningful topics in less than hundred gibbs sampling steps. The `topic0` seems to talk about **car crashes**, the `topic9` is clearly about **Iraq** ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This project is very educational. Not only did it allow us to deepen our knowledge of Spark, but also to implement one of the most widely used topic modeling algorithms.\n",
    "\n",
    ">The introduction of Gibbs sampling in parallel by Wang & Al. was enable us to bypass the sequential nature of MCMC algorithms and take advantage of the power of tools such as Spark.\n",
    "\n",
    ">A next step would have been to be able to analyze the speed-up gained via this parallelized scheme."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
