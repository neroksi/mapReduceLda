{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "\n",
    "* As the gaps between global updates get higher, the gibbs convergence to the true dist might become worst. How can we test this idea ? By looking at the number of simulation needed befor convergence ?\n",
    "\n",
    "* As the gaps between global updates get higher, the number of communications will decrease and thus the algorithm will become  faster. Can we test that idea ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Row\n",
    "from pyspark import TaskContext\n",
    "from pyspark import AccumulatorParam\n",
    "from pyspark.sql  import functions as F\n",
    "from pyspark.sql.types import StructType,StructField, FloatType ,IntegerType, StringType, MapType, ArrayType,LongType\n",
    "# from pyspark.sql.functions import UserDefinedFunction as udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "import numpy as np, os, shutil, json, time\n",
    "import pickle as pkl\n",
    "from datetime import datetime\n",
    "from prep import preprocessAndGetTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"lda\") \\\n",
    "    .getOrCreate()#.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAbstract(docstr):\n",
    "    doc = json.loads(docstr)\n",
    "    abstract = doc[\"paperAbstract\"]\n",
    "    tokens = np.array(preprocessAndGetTokens(abstract))\n",
    "    return (doc[\"id\"], tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.sparkContext.textFile(\"sample-S2-records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('4cbba8127c8747a3b2cfb9c1f48c43e5c15e323e',\n",
       "  array(['unit', 'analysi', 'compar', 'includ', 'histor', 'decemb',\n",
       "         'surgeri', 'subclassifi', 'paradigm', 'chemotherapi', 'treatment',\n",
       "         'diagnosi', 'cancer', 'trend', 'complet', 'januari', 'appear',\n",
       "         'admiss', 'recent', 'final', 'toward', 'complic', 'result',\n",
       "         'accord', 'receiv', 'resect', 'ovarian', 'frequenc', 'os',\n",
       "         'support', 'treat', 'method', 'outcom', 'use', 'versus', 'patient',\n",
       "         'postop', 'extract', 'evalu', 'time', 'standard', 'diagnos',\n",
       "         'tumor', 'nactid', 'retrospect', 'achiev', 'overal', 'higher',\n",
       "         'kaplanmei', 'vs', 'record', 'advanc', 'diseas', 'iv', 'pds',\n",
       "         'shift', 'site', 'stage', 'identifi', 'neoadjuv', 'intens', 'like',\n",
       "         'residu', 'epitheli', 'less', 'benefit', 'hypothes', 'total',\n",
       "         'longer', 'group', 'underw', 'month', 'surviv', 'median',\n",
       "         'dissemin', 'data', 'signific', 'interv', 'morbid', 'similar',\n",
       "         'newli', 'frequent', 'would', 'women', 'rate', 'care', 'primari',\n",
       "         'day', 'debulk', 'equival'], dtype='<U12'))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = data.map(processAbstract).filter(lambda x : len(x[1]) > 0)\n",
    "corpus.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with open(path, \"rb\") as f :\n",
    "        obj = pkl.load(f)\n",
    "    return obj\n",
    "\n",
    "def dump(obj,path, mode = \"wb\"):\n",
    "    with open(path, mode) as f :\n",
    "        pkl.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveByPartition(ind, part, folder = \"matrix/corpusTopic\", mode = \"wb\", batchsize = 10):\n",
    "    root = folder + \"/partition__%04d__\"%ind\n",
    "    if os.path.exists(root):\n",
    "        if mode == \"wb\":\n",
    "            shutil.rmtree(root)\n",
    "    os.mkdir(root)\n",
    "    write_more = True\n",
    "    counter = 0\n",
    "    while write_more :\n",
    "        write_more = False\n",
    "        file = root + \"/batch_%010d\"%counter\n",
    "        nwrited = 0\n",
    "        with open(file, mode ) as f :\n",
    "            for el in part :\n",
    "                pkl.dump(el, f)\n",
    "                nwrited += 1\n",
    "                write_more = True\n",
    "                if nwrited >= batchsize :\n",
    "                    break\n",
    "            \n",
    "            counter += 1\n",
    "        if not write_more :\n",
    "            os.remove(file)\n",
    "            break\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVocabulary(corpus_rdd,npPartitons,  mode = \"wb\"):\n",
    "    vocabByPartition = corpus_rdd\\\n",
    "                .mapValues(lambda x : x[1]).reduceByKey(lambda x, y : np.union1d(x,y) )\\\n",
    "                    .collect()\n",
    "    \n",
    "    vocabAll = np.unique(np.concatenate([ v[1] for v in vocabByPartition]))\n",
    "    vocabAll =  {w:ind for w,ind in zip(vocabAll, range(len(vocabAll))) }\n",
    "    with open(\"matrix/vocabulary/vocabAll\", mode) as f :\n",
    "        pkl.dump(vocabAll, f)\n",
    "    \n",
    "    for v in vocabByPartition :\n",
    "        wLocIdGlobId = {w : (ind, vocabAll[w]) for w, ind in zip(v[1], range(len(v[1]))) }\n",
    "        with open(\"matrix/vocabulary/vocab__%04d__\"%v[0], mode) as f :\n",
    "            pkl.dump(wLocIdGlobId, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDocsMap(corpus_rdd,npPartitons, mode = \"wb\"):\n",
    "    docsByPartion = corpus_rdd.mapValues(lambda x : x[0] )\\\n",
    "                .groupByKey().mapValues(list).collect()\n",
    "    \n",
    "    for v in docsByPartion :\n",
    "        partDocLocId = {doc :ind for doc, ind in zip(v[1], range(len(v[1]))) }\n",
    "        with open(\"matrix/docsMap/docs__%04d__\"%v[0], mode) as f :\n",
    "            pkl.dump(partDocLocId, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbPartitions = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = corpus.map(lambda x : (np.random.choice(nbPartitions), x))\\\n",
    "            .partitionBy(nbPartitions).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus2.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeVocabulary(corpus2, nbPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeDocsMap(corpus2, nbPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2701"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load(\"matrix/vocabulary/vocabAll\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load(\"matrix/vocabulary/vocab__0000__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(load(\"matrix/docsMap/docs__0000__\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1934"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabAll = load(\"matrix/vocabulary/vocabAll\")\n",
    "vocabAll['provid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countWords2 = corpTop.flatMap(lambda x : [(u,1) for u in zip(x[1], x[2])]).countByKey()\n",
    "# coo = np.array(list(countWords2.keys()))\n",
    "# data = np.array(list(countWords2.values()))\n",
    "# order = coo[:, 0].argsort()\n",
    "# countWords2 = coo_matrix((data[order] , (coo[order, 0], coo[order, 1])), shape = (nbVocab, nbTopics)).toarray()\n",
    "# countWords2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# àà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initCountWordsAll():\n",
    "    countWords = np.zeros((len(vocabAll), nbTopics))\n",
    "    for ind in range(nbPartitions):\n",
    "        countWords_ind = load(\"matrix/countWords/words__%04d__\"%ind)\n",
    "        table = np.array(list(vocabs[ind].values()))\n",
    "        countWords[table[:,1]] += countWords_ind[table[:,0]]\n",
    "    dump(countWords , \"matrix/countWords/words_all\")\n",
    "    \n",
    "    #Now let update each countWords to its correct state (the old values are wrong as they\n",
    "    # don't take into account the docs in other partitions)\n",
    "    for ind in range(nbPartitions):\n",
    "        table = np.array(list(vocabs[ind].values()))\n",
    "        dump(countWords[table[:,1]], \"matrix/countWords/words__%04d__\"%ind)\n",
    "    \n",
    "    \n",
    "def updateCountWordsAll():\n",
    "    countWords = load( \"matrix/countWords/words_all\")\n",
    "\n",
    "    for ind in range(nbPartitions):\n",
    "        deltaWords = load(\"matrix/deltaWords/deltas__%04d__\"%ind)\n",
    "        table = np.array(list(vocabs[ind].values()))\n",
    "        countWords[table[:,1]] += deltaWords[table[:, 0]]\n",
    "    dump(countWords , \"matrix/countWords/words_all\")\n",
    "    \n",
    "    #Now let update each countWords to its correct state (the old values are wrong as they\n",
    "    # don't take into account the docs in other partitions)\n",
    "    for ind in range(nbPartitions):\n",
    "        table = np.array(list(vocabs[ind].values()))\n",
    "        order = np.argsort(table[:, 0])\n",
    "        table = table[order]\n",
    "        dump(countWords[table[:,1]], \"matrix/countWords/words__%04d__\"%ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.map(lambda x : (len(x[1]) == len(np.unique(x[1])))*1).collect() #check if unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq = corpus.flatMap(lambda x : [(w, 1) for w in x[1]]).countByKey()\n",
    "# sum(freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abdomin': 0,\n",
       " 'aber': 1,\n",
       " 'abhiingig': 2,\n",
       " 'abil': 3,\n",
       " 'abl': 4,\n",
       " 'abnorm': 5,\n",
       " 'abolish': 6,\n",
       " 'absenc': 7,\n",
       " 'absent': 8,\n",
       " 'absolut': 9,\n",
       " 'absorb': 10,\n",
       " 'absorpt': 11,\n",
       " 'abund': 12,\n",
       " 'academ': 13,\n",
       " 'academi': 14,\n",
       " 'acaij': 15,\n",
       " 'acceler': 16,\n",
       " 'accept': 17,\n",
       " 'access': 18,\n",
       " 'accesscontrol': 19,\n",
       " 'accomplish': 20,\n",
       " 'accord': 21,\n",
       " 'account': 22,\n",
       " 'accumul': 23,\n",
       " 'accur': 24,\n",
       " 'accuraci': 25,\n",
       " 'acet': 26,\n",
       " 'achiev': 27,\n",
       " 'acid': 28,\n",
       " 'acidif': 29,\n",
       " 'acknowledg': 30,\n",
       " 'acquir': 31,\n",
       " 'acquisit': 32,\n",
       " 'across': 33,\n",
       " 'act': 34,\n",
       " 'action': 35,\n",
       " 'activ': 36,\n",
       " 'acut': 37,\n",
       " 'adapt': 38,\n",
       " 'adc': 39,\n",
       " 'add': 40,\n",
       " 'addit': 41,\n",
       " 'address': 42,\n",
       " 'adequ': 43,\n",
       " 'adher': 44,\n",
       " 'adhes': 45,\n",
       " 'adipos': 46,\n",
       " 'adjust': 47,\n",
       " 'adjuv': 48,\n",
       " 'administ': 49,\n",
       " 'admiss': 50,\n",
       " 'adopt': 51,\n",
       " 'adrenalin': 52,\n",
       " 'adult': 53,\n",
       " 'advanc': 54,\n",
       " 'advantag': 55,\n",
       " 'advers': 56,\n",
       " 'advisor': 57,\n",
       " 'aemail': 58,\n",
       " 'aep': 59,\n",
       " 'aerial': 60,\n",
       " 'aethiop': 61,\n",
       " 'affect': 62,\n",
       " 'affin': 63,\n",
       " 'african': 64,\n",
       " 'ag': 65,\n",
       " 'agalnst': 66,\n",
       " 'agaros': 67,\n",
       " 'age': 68,\n",
       " 'agent': 69,\n",
       " 'agre': 70,\n",
       " 'agreement': 71,\n",
       " 'ahead': 72,\n",
       " 'aid': 73,\n",
       " 'aim': 74,\n",
       " 'air': 75,\n",
       " 'al': 76,\n",
       " 'alba': 77,\n",
       " 'alda': 78,\n",
       " 'algebra': 79,\n",
       " 'algorithm': 80,\n",
       " 'align': 81,\n",
       " 'alkalin': 82,\n",
       " 'aller': 83,\n",
       " 'alloc': 84,\n",
       " 'allodynia': 85,\n",
       " 'allograft': 86,\n",
       " 'allopurinol': 87,\n",
       " 'allow': 88,\n",
       " 'alloy': 89,\n",
       " 'alon': 90,\n",
       " 'along': 91,\n",
       " 'also': 92,\n",
       " 'although': 93,\n",
       " 'amblyraja': 94,\n",
       " 'amelior': 95,\n",
       " 'amen': 96,\n",
       " 'american': 97,\n",
       " 'americana': 98,\n",
       " 'aminereact': 99,\n",
       " 'amitriptylin': 100,\n",
       " 'among': 101,\n",
       " 'amorph': 102,\n",
       " 'amount': 103,\n",
       " 'amphotericin': 104,\n",
       " 'amplitud': 105,\n",
       " 'anaesthet': 106,\n",
       " 'analges': 107,\n",
       " 'analgesia': 108,\n",
       " 'analogu': 109,\n",
       " 'analys': 110,\n",
       " 'analysi': 111,\n",
       " 'analyt': 112,\n",
       " 'analyz': 113,\n",
       " 'anaphylact': 114,\n",
       " 'anaphylaxi': 115,\n",
       " 'anatom': 116,\n",
       " 'anatomybas': 117,\n",
       " 'andor': 118,\n",
       " 'angewandt': 119,\n",
       " 'angioedema': 120,\n",
       " 'angiographi': 121,\n",
       " 'angl': 122,\n",
       " 'anim': 123,\n",
       " 'annual': 124,\n",
       " 'anomal': 125,\n",
       " 'anomali': 126,\n",
       " 'anoth': 127,\n",
       " 'anova': 128,\n",
       " 'answer': 129,\n",
       " 'anteced': 130,\n",
       " 'antenna': 131,\n",
       " 'anterior': 132,\n",
       " 'antiapoptot': 133,\n",
       " 'antibodi': 134,\n",
       " 'anticarcinogen': 135,\n",
       " 'antidepress': 136,\n",
       " 'antigen': 137,\n",
       " 'antihypertens': 138,\n",
       " 'anyon': 139,\n",
       " 'apertur': 140,\n",
       " 'apoptosi': 141,\n",
       " 'appar': 142,\n",
       " 'appear': 143,\n",
       " 'appendix': 144,\n",
       " 'appleton': 145,\n",
       " 'appli': 146,\n",
       " 'applic': 147,\n",
       " 'appreci': 148,\n",
       " 'approach': 149,\n",
       " 'appropri': 150,\n",
       " 'april': 151,\n",
       " 'apt': 152,\n",
       " 'aqueous': 153,\n",
       " 'arbeitsbedingungen': 154,\n",
       " 'arbeitsptatz': 155,\n",
       " 'area': 156,\n",
       " 'arid': 157,\n",
       " 'arnel': 158,\n",
       " 'aromaten': 159,\n",
       " 'around': 160,\n",
       " 'array': 161,\n",
       " 'arrhythmia': 162,\n",
       " 'arri': 163,\n",
       " 'art': 164,\n",
       " 'arteri': 165,\n",
       " 'aspect': 166,\n",
       " 'assay': 167,\n",
       " 'assess': 168,\n",
       " 'assist': 169,\n",
       " 'associ': 170,\n",
       " 'asthma': 171,\n",
       " 'asymmetr': 172,\n",
       " 'atherosclerosi': 173,\n",
       " 'athlet': 174,\n",
       " 'atm': 175,\n",
       " 'atmbas': 176,\n",
       " 'atom': 177,\n",
       " 'atomicelectron': 178,\n",
       " 'atp': 179,\n",
       " 'atresia': 180,\n",
       " 'attach': 181,\n",
       " 'attack': 182,\n",
       " 'attain': 183,\n",
       " 'attempt': 184,\n",
       " 'attend': 185,\n",
       " 'attent': 186,\n",
       " 'attitud': 187,\n",
       " 'auch': 188,\n",
       " 'audio': 189,\n",
       " 'auditori': 190,\n",
       " 'auf': 191,\n",
       " 'aural': 192,\n",
       " 'australia': 193,\n",
       " 'automat': 194,\n",
       " 'automata': 195,\n",
       " 'automorph': 196,\n",
       " 'automot': 197,\n",
       " 'autopsi': 198,\n",
       " 'autosampl': 199,\n",
       " 'avail': 200,\n",
       " 'avc': 201,\n",
       " 'averag': 202,\n",
       " 'avoid': 203,\n",
       " 'awaken': 204,\n",
       " 'awar': 205,\n",
       " 'axi': 206,\n",
       " 'back': 207,\n",
       " 'background': 208,\n",
       " 'bad': 209,\n",
       " 'ball': 210,\n",
       " 'band': 211,\n",
       " 'bandgap': 212,\n",
       " 'bandwidth': 213,\n",
       " 'base': 214,\n",
       " 'basedon': 215,\n",
       " 'basic': 216,\n",
       " 'basketbal': 217,\n",
       " 'bath': 218,\n",
       " 'bati': 219,\n",
       " 'batteri': 220,\n",
       " 'beachten': 221,\n",
       " 'beam': 222,\n",
       " 'beat': 223,\n",
       " 'becom': 224,\n",
       " 'bed': 225,\n",
       " 'bedenken': 226,\n",
       " 'beer': 227,\n",
       " 'behav': 228,\n",
       " 'behavior': 229,\n",
       " 'behaviour': 230,\n",
       " 'bei': 231,\n",
       " 'beider': 232,\n",
       " 'beij': 233,\n",
       " 'belief': 234,\n",
       " 'believ': 235,\n",
       " 'belong': 236,\n",
       " 'bemail': 237,\n",
       " 'benefi': 238,\n",
       " 'benefici': 239,\n",
       " 'benefit': 240,\n",
       " 'bent': 241,\n",
       " 'benzol': 242,\n",
       " 'benzolanteil': 243,\n",
       " 'beriehtet': 244,\n",
       " 'best': 245,\n",
       " 'bestimmten': 246,\n",
       " 'betaactin': 247,\n",
       " 'betahematin': 248,\n",
       " 'better': 249,\n",
       " 'betweenbatch': 250,\n",
       " 'bid': 251,\n",
       " 'biject': 252,\n",
       " 'bilater': 253,\n",
       " 'binari': 254,\n",
       " 'bind': 255,\n",
       " 'biochem': 256,\n",
       " 'biodegrad': 257,\n",
       " 'biolog': 258,\n",
       " 'biomark': 259,\n",
       " 'biomed': 260,\n",
       " 'biomphalaria': 261,\n",
       " 'biopsi': 262,\n",
       " 'birefring': 263,\n",
       " 'birth': 264,\n",
       " 'bisimul': 265,\n",
       " 'bk': 266,\n",
       " 'black': 267,\n",
       " 'blink': 268,\n",
       " 'block': 269,\n",
       " 'blocker': 270,\n",
       " 'blood': 271,\n",
       " 'bloodpressurelow': 272,\n",
       " 'bluetooth': 273,\n",
       " 'blutbildenden': 274,\n",
       " 'bodi': 275,\n",
       " 'bond': 276,\n",
       " 'borrow': 277,\n",
       " 'bortezomib': 278,\n",
       " 'bortezomibinduc': 279,\n",
       " 'boson': 280,\n",
       " 'bottleneck': 281,\n",
       " 'bottom': 282,\n",
       " 'bpe': 283,\n",
       " 'brachyura': 284,\n",
       " 'brain': 285,\n",
       " 'brainstem': 286,\n",
       " 'break': 287,\n",
       " 'breakinduc': 288,\n",
       " 'brealc': 289,\n",
       " 'breath': 290,\n",
       " 'bridg': 291,\n",
       " 'bring': 292,\n",
       " 'brisban': 293,\n",
       " 'british': 294,\n",
       " 'broad': 295,\n",
       " 'broaden': 296,\n",
       " 'bromid': 297,\n",
       " 'brownian': 298,\n",
       " 'built': 299,\n",
       " 'bulk': 300,\n",
       " 'burn': 301,\n",
       " 'busard': 302,\n",
       " 'bv': 303,\n",
       " 'bw': 304,\n",
       " 'ca': 305,\n",
       " 'cach': 306,\n",
       " 'cactiv': 307,\n",
       " 'cad': 308,\n",
       " 'caffein': 309,\n",
       " 'cag': 310,\n",
       " 'calcul': 311,\n",
       " 'calculus': 312,\n",
       " 'calibr': 313,\n",
       " 'call': 314,\n",
       " 'calorimet': 315,\n",
       " 'calorimetr': 316,\n",
       " 'came': 317,\n",
       " 'camera': 318,\n",
       " 'campaign': 319,\n",
       " 'cancer': 320,\n",
       " 'candid': 321,\n",
       " 'canicula': 322,\n",
       " 'capabl': 323,\n",
       " 'capac': 324,\n",
       " 'capacit': 325,\n",
       " 'caproni': 326,\n",
       " 'capsul': 327,\n",
       " 'captur': 328,\n",
       " 'carbon': 329,\n",
       " 'carcinoma': 330,\n",
       " 'cardiac': 331,\n",
       " 'cardiomyopathi': 332,\n",
       " 'cardiovascular': 333,\n",
       " 'care': 334,\n",
       " 'carmin': 335,\n",
       " 'carri': 336,\n",
       " 'case': 337,\n",
       " 'cat': 338,\n",
       " 'catalas': 339,\n",
       " 'catastroph': 340,\n",
       " 'catechin': 341,\n",
       " 'categor': 342,\n",
       " 'cathet': 343,\n",
       " 'caudal': 344,\n",
       " 'caus': 345,\n",
       " 'causal': 346,\n",
       " 'cci': 347,\n",
       " 'cciinduc': 348,\n",
       " 'cd': 349,\n",
       " 'cdatp': 350,\n",
       " 'cdexpress': 351,\n",
       " 'cdhigh': 352,\n",
       " 'cdlow': 353,\n",
       " 'cell': 354,\n",
       " 'cellular': 355,\n",
       " 'cencentr': 356,\n",
       " 'cent': 357,\n",
       " 'center': 358,\n",
       " 'centerspecif': 359,\n",
       " 'centr': 360,\n",
       " 'central': 361,\n",
       " 'cercopithecus': 362,\n",
       " 'cerebrovascular': 363,\n",
       " 'certain': 364,\n",
       " 'cervicomedullari': 365,\n",
       " 'cf': 366,\n",
       " 'cflip': 367,\n",
       " 'chain': 368,\n",
       " 'challeng': 369,\n",
       " 'championship': 370,\n",
       " 'chang': 371,\n",
       " 'channel': 372,\n",
       " 'chapter': 373,\n",
       " 'charact': 374,\n",
       " 'character': 375,\n",
       " 'characterist': 376,\n",
       " 'charg': 377,\n",
       " 'chargedischarg': 378,\n",
       " 'chargingdischarg': 379,\n",
       " 'charl': 380,\n",
       " 'cheapli': 381,\n",
       " 'chemic': 382,\n",
       " 'chemistri': 383,\n",
       " 'chemotherapi': 384,\n",
       " 'children': 385,\n",
       " 'chimera': 386,\n",
       " 'china': 387,\n",
       " 'chines': 388,\n",
       " 'chip': 389,\n",
       " 'chloraminet': 390,\n",
       " 'chloraminetcat': 391,\n",
       " 'choic': 392,\n",
       " 'cholesterol': 393,\n",
       " 'chromatin': 394,\n",
       " 'chromatographi': 395,\n",
       " 'chronic': 396,\n",
       " 'ci': 397,\n",
       " 'cient': 398,\n",
       " 'cigarett': 399,\n",
       " 'circuit': 400,\n",
       " 'circuitri': 401,\n",
       " 'circul': 402,\n",
       " 'circular': 403,\n",
       " 'circumst': 404,\n",
       " 'cite': 405,\n",
       " 'claim': 406,\n",
       " 'class': 407,\n",
       " 'classic': 408,\n",
       " 'clavata': 409,\n",
       " 'clchenphyssinicaedutw': 410,\n",
       " 'clean': 411,\n",
       " 'clear': 412,\n",
       " 'clearanc': 413,\n",
       " 'cleav': 414,\n",
       " 'clifford': 415,\n",
       " 'climat': 416,\n",
       " 'clinic': 417,\n",
       " 'clone': 418,\n",
       " 'close': 419,\n",
       " 'closur': 420,\n",
       " 'cluster': 421,\n",
       " 'cm': 422,\n",
       " 'cmos': 423,\n",
       " 'co': 424,\n",
       " 'coach': 425,\n",
       " 'coadminist': 426,\n",
       " 'coast': 427,\n",
       " 'coastal': 428,\n",
       " 'cobalt': 429,\n",
       " 'cochlear': 430,\n",
       " 'cockroach': 431,\n",
       " 'coco': 432,\n",
       " 'code': 433,\n",
       " 'coe': 434,\n",
       " 'coeffici': 435,\n",
       " 'cofactor': 436,\n",
       " 'coher': 437,\n",
       " 'coincid': 438,\n",
       " 'collagen': 439,\n",
       " 'colleagu': 440,\n",
       " 'collect': 441,\n",
       " 'colonoscopi': 442,\n",
       " 'colorect': 443,\n",
       " 'colour': 444,\n",
       " 'column': 445,\n",
       " 'coma': 446,\n",
       " 'comatos': 447,\n",
       " 'combin': 448,\n",
       " 'comet': 449,\n",
       " 'commerci': 450,\n",
       " 'comminut': 451,\n",
       " 'common': 452,\n",
       " 'communic': 453,\n",
       " 'communiti': 454,\n",
       " 'commut': 455,\n",
       " 'compani': 456,\n",
       " 'compar': 457,\n",
       " 'comparison': 458,\n",
       " 'compart': 459,\n",
       " 'compens': 460,\n",
       " 'compet': 461,\n",
       " 'compil': 462,\n",
       " 'complet': 463,\n",
       " 'complex': 464,\n",
       " 'complic': 465,\n",
       " 'compon': 466,\n",
       " 'compound': 467,\n",
       " 'comprehens': 468,\n",
       " 'compress': 469,\n",
       " 'comput': 470,\n",
       " 'concentr': 471,\n",
       " 'concern': 472,\n",
       " 'concert': 473,\n",
       " 'conclud': 474,\n",
       " 'conclus': 475,\n",
       " 'condens': 476,\n",
       " 'condit': 477,\n",
       " 'condom': 478,\n",
       " 'conduct': 479,\n",
       " 'confer': 480,\n",
       " 'confi': 481,\n",
       " 'confid': 482,\n",
       " 'configur': 483,\n",
       " 'confirm': 484,\n",
       " 'congest': 485,\n",
       " 'conjug': 486,\n",
       " 'connect': 487,\n",
       " 'consecut': 488,\n",
       " 'consequ': 489,\n",
       " 'consid': 490,\n",
       " 'consider': 491,\n",
       " 'consist': 492,\n",
       " 'constant': 493,\n",
       " 'constraint': 494,\n",
       " 'constrict': 495,\n",
       " 'consumpt': 496,\n",
       " 'consumptioni': 497,\n",
       " 'consumptionin': 498,\n",
       " 'contain': 499,\n",
       " 'content': 500,\n",
       " 'context': 501,\n",
       " 'contextu': 502,\n",
       " 'continu': 503,\n",
       " 'contract': 504,\n",
       " 'contrari': 505,\n",
       " 'contrarili': 506,\n",
       " 'contrast': 507,\n",
       " 'contribut': 508,\n",
       " 'control': 509,\n",
       " 'conveni': 510,\n",
       " 'convent': 511,\n",
       " 'convers': 512,\n",
       " 'convert': 513,\n",
       " 'cooo': 514,\n",
       " 'coordin': 515,\n",
       " 'copyright': 516,\n",
       " 'coq': 517,\n",
       " 'cord': 518,\n",
       " 'core': 519,\n",
       " 'coronari': 520,\n",
       " 'corpor': 521,\n",
       " 'correct': 522,\n",
       " 'correl': 523,\n",
       " 'correspond': 524,\n",
       " 'corsodyl': 525,\n",
       " 'cortex': 526,\n",
       " 'cortic': 527,\n",
       " 'cosi': 528,\n",
       " 'cosixgex': 529,\n",
       " 'cost': 530,\n",
       " 'could': 531,\n",
       " 'counsel': 532,\n",
       " 'countri': 533,\n",
       " 'coupl': 534,\n",
       " 'coval': 535,\n",
       " 'cover': 536,\n",
       " 'coverag': 537,\n",
       " 'cpd': 538,\n",
       " 'cpu': 539,\n",
       " 'crc': 540,\n",
       " 'creat': 541,\n",
       " 'crisi': 542,\n",
       " 'critic': 543,\n",
       " 'cross': 544,\n",
       " 'crosslink': 545,\n",
       " 'crosssect': 546,\n",
       " 'crotalus': 547,\n",
       " 'crucial': 548,\n",
       " 'crystal': 549,\n",
       " 'ct': 550,\n",
       " 'cuau': 551,\n",
       " 'cue': 552,\n",
       " 'cultur': 553,\n",
       " 'cure': 554,\n",
       " 'current': 555,\n",
       " 'curvilinear': 556,\n",
       " 'cycl': 557,\n",
       " 'cystein': 558,\n",
       " 'cytomegaloviremia': 559,\n",
       " 'cytometri': 560,\n",
       " 'cytoplasm': 561,\n",
       " 'cytotox': 562,\n",
       " 'da': 563,\n",
       " 'dabei': 564,\n",
       " 'dahingestellt': 565,\n",
       " 'daili': 566,\n",
       " 'dark': 567,\n",
       " 'das': 568,\n",
       " 'data': 569,\n",
       " 'databas': 570,\n",
       " 'dataset': 571,\n",
       " 'date': 572,\n",
       " 'day': 573,\n",
       " 'dazu': 574,\n",
       " 'db': 575,\n",
       " 'de': 576,\n",
       " 'deal': 577,\n",
       " 'death': 578,\n",
       " 'debrid': 579,\n",
       " 'debulk': 580,\n",
       " 'decellular': 581,\n",
       " 'decemb': 582,\n",
       " 'decerebr': 583,\n",
       " 'decis': 584,\n",
       " 'decisionmak': 585,\n",
       " 'declar': 586,\n",
       " 'declin': 587,\n",
       " 'decreas': 588,\n",
       " 'deduc': 589,\n",
       " 'deem': 590,\n",
       " 'defect': 591,\n",
       " 'defici': 592,\n",
       " 'defin': 593,\n",
       " 'definit': 594,\n",
       " 'deform': 595,\n",
       " 'degc': 596,\n",
       " 'degrad': 597,\n",
       " 'degre': 598,\n",
       " 'delay': 599,\n",
       " 'delaysarenot': 600,\n",
       " 'deliv': 601,\n",
       " 'deliveri': 602,\n",
       " 'dem': 603,\n",
       " 'demand': 604,\n",
       " 'democraci': 605,\n",
       " 'demonstr': 606,\n",
       " 'den': 607,\n",
       " 'densiti': 608,\n",
       " 'dental': 609,\n",
       " 'dentat': 610,\n",
       " 'depart': 611,\n",
       " 'depend': 612,\n",
       " 'depress': 613,\n",
       " 'depth': 614,\n",
       " 'der': 615,\n",
       " 'deriv': 616,\n",
       " 'des': 617,\n",
       " 'describ': 618,\n",
       " 'descript': 619,\n",
       " 'deserv': 620,\n",
       " 'design': 621,\n",
       " 'desir': 622,\n",
       " 'desmethylzopiclon': 623,\n",
       " 'desorpt': 624,\n",
       " 'despit': 625,\n",
       " 'destin': 626,\n",
       " 'destruct': 627,\n",
       " 'detail': 628,\n",
       " 'detect': 629,\n",
       " 'deterg': 630,\n",
       " 'determin': 631,\n",
       " 'determinist': 632,\n",
       " 'develop': 633,\n",
       " 'development': 634,\n",
       " 'deviat': 635,\n",
       " 'devic': 636,\n",
       " 'devicescan': 637,\n",
       " 'diabet': 638,\n",
       " 'diagnos': 639,\n",
       " 'diagnosi': 640,\n",
       " 'diagnost': 641,\n",
       " 'diamet': 642,\n",
       " 'diastol': 643,\n",
       " 'die': 644,\n",
       " 'dies': 645,\n",
       " 'differ': 646,\n",
       " 'differenti': 647,\n",
       " 'difficult': 648,\n",
       " 'digenesi': 649,\n",
       " 'digit': 650,\n",
       " 'dimer': 651,\n",
       " 'dimerspecif': 652,\n",
       " 'diminut': 653,\n",
       " 'dinitrophenol': 654,\n",
       " 'dioxid': 655,\n",
       " 'dipturus': 656,\n",
       " 'direct': 657,\n",
       " 'disabl': 658,\n",
       " 'discontinu': 659,\n",
       " 'discov': 660,\n",
       " 'discrep': 661,\n",
       " 'discret': 662,\n",
       " 'discuss': 663,\n",
       " 'diseas': 664,\n",
       " 'disfigur': 665,\n",
       " 'disk': 666,\n",
       " 'dismutas': 667,\n",
       " 'disord': 668,\n",
       " 'displac': 669,\n",
       " 'display': 670,\n",
       " 'disproportion': 671,\n",
       " 'dissemin': 672,\n",
       " 'distal': 673,\n",
       " 'distanc': 674,\n",
       " 'distant': 675,\n",
       " 'distort': 676,\n",
       " 'distractor': 677,\n",
       " 'distribut': 678,\n",
       " 'diurnal': 679,\n",
       " 'diverg': 680,\n",
       " 'divers': 681,\n",
       " 'dna': 682,\n",
       " 'dnas': 683,\n",
       " 'dnrch': 684,\n",
       " 'doi': 685,\n",
       " 'domain': 686,\n",
       " 'domainlik': 687,\n",
       " 'done': 688,\n",
       " 'dongclnsrrcorgtw': 689,\n",
       " 'dope': 690,\n",
       " 'dorsal': 691,\n",
       " 'dos': 692,\n",
       " 'dosag': 693,\n",
       " 'dose': 694,\n",
       " 'doselevel': 695,\n",
       " 'dot': 696,\n",
       " 'dquantum': 697,\n",
       " 'drainag': 698,\n",
       " 'drawback': 699,\n",
       " 'drawn': 700,\n",
       " 'dri': 701,\n",
       " 'drive': 702,\n",
       " 'driver': 703,\n",
       " 'drosophila': 704,\n",
       " 'drug': 705,\n",
       " 'due': 706,\n",
       " 'duloxetin': 707,\n",
       " 'durat': 708,\n",
       " 'durissus': 709,\n",
       " 'dv': 710,\n",
       " 'dvs': 711,\n",
       " 'dye': 712,\n",
       " 'dyecoupl': 713,\n",
       " 'dyeinject': 714,\n",
       " 'dynam': 715,\n",
       " 'dysfunct': 716,\n",
       " 'ear': 717,\n",
       " 'earli': 718,\n",
       " 'earlier': 719,\n",
       " 'earlylat': 720,\n",
       " 'earth': 721,\n",
       " 'easili': 722,\n",
       " 'eastern': 723,\n",
       " 'eb': 724,\n",
       " 'echinocandin': 725,\n",
       " 'echinostom': 726,\n",
       " 'echinostoma': 727,\n",
       " 'echo': 728,\n",
       " 'ecm': 729,\n",
       " 'ecmbas': 730,\n",
       " 'ecotyp': 731,\n",
       " 'ect': 732,\n",
       " 'ecuador': 733,\n",
       " 'educ': 734,\n",
       " 'ef': 735,\n",
       " 'eff': 736,\n",
       " 'effahrnngsgema': 737,\n",
       " 'effect': 738,\n",
       " 'effici': 739,\n",
       " 'efllect': 740,\n",
       " 'eftect': 741,\n",
       " 'eg': 742,\n",
       " 'egcg': 743,\n",
       " 'egflik': 744,\n",
       " 'eggcas': 745,\n",
       " 'eighth': 746,\n",
       " 'ein': 747,\n",
       " 'einem': 748,\n",
       " 'either': 749,\n",
       " 'elasmobranch': 750,\n",
       " 'elast': 751,\n",
       " 'elastasedigest': 752,\n",
       " 'elay': 753,\n",
       " 'electr': 754,\n",
       " 'electrephoresi': 755,\n",
       " 'electrocardiographi': 756,\n",
       " 'electrod': 757,\n",
       " 'electron': 758,\n",
       " 'electrophoresi': 759,\n",
       " 'electrophysiolog': 760,\n",
       " 'electrospray': 761,\n",
       " 'element': 762,\n",
       " 'elementari': 763,\n",
       " 'elev': 764,\n",
       " 'elig': 765,\n",
       " 'elimin': 766,\n",
       " 'ellect': 767,\n",
       " 'elsevi': 768,\n",
       " 'embed': 769,\n",
       " 'emerg': 770,\n",
       " 'emiss': 771,\n",
       " 'emit': 772,\n",
       " 'emitt': 773,\n",
       " 'emot': 774,\n",
       " 'emphas': 775,\n",
       " 'empir': 776,\n",
       " 'employ': 777,\n",
       " 'employe': 778,\n",
       " 'employeeemploy': 779,\n",
       " 'empti': 780,\n",
       " 'enabl': 781,\n",
       " 'encapsul': 782,\n",
       " 'enceph': 783,\n",
       " 'encod': 784,\n",
       " 'encourag': 785,\n",
       " 'end': 786,\n",
       " 'endem': 787,\n",
       " 'endocrin': 788,\n",
       " 'endonucleas': 789,\n",
       " 'endtoendpacket': 790,\n",
       " 'energi': 791,\n",
       " 'engag': 792,\n",
       " 'engin': 793,\n",
       " 'england': 794,\n",
       " 'english': 795,\n",
       " 'enhanc': 796,\n",
       " 'enjoy': 797,\n",
       " 'enough': 798,\n",
       " 'enrol': 799,\n",
       " 'entail': 800,\n",
       " 'enter': 801,\n",
       " 'enterpris': 802,\n",
       " 'entiti': 803,\n",
       " 'environ': 804,\n",
       " 'environment': 805,\n",
       " 'enzym': 806,\n",
       " 'enzymat': 807,\n",
       " 'epidem': 808,\n",
       " 'epiderm': 809,\n",
       " 'epigallocatechin': 810,\n",
       " 'episod': 811,\n",
       " 'epitheli': 812,\n",
       " 'epizoodem': 813,\n",
       " 'epla': 814,\n",
       " 'equal': 815,\n",
       " 'equat': 816,\n",
       " 'equin': 817,\n",
       " 'equip': 818,\n",
       " 'equival': 819,\n",
       " 'er': 820,\n",
       " 'erhobenen': 821,\n",
       " 'erk': 822,\n",
       " 'erreicht': 823,\n",
       " 'error': 824,\n",
       " 'errorpron': 825,\n",
       " 'erscheint': 826,\n",
       " 'erythematosus': 827,\n",
       " 'erythrocyt': 828,\n",
       " 'es': 829,\n",
       " 'esculentum': 830,\n",
       " 'especi': 831,\n",
       " 'essenti': 832,\n",
       " 'establish': 833,\n",
       " 'estim': 834,\n",
       " 'estimatesof': 835,\n",
       " 'estrogen': 836,\n",
       " 'estrus': 837,\n",
       " 'et': 838,\n",
       " 'ethidium': 839,\n",
       " 'ethnic': 840,\n",
       " 'etiolog': 841,\n",
       " 'evalu': 842,\n",
       " 'evapor': 843,\n",
       " 'even': 844,\n",
       " 'event': 845,\n",
       " 'eventu': 846,\n",
       " 'everi': 847,\n",
       " 'evid': 848,\n",
       " 'evidencebas': 849,\n",
       " 'evok': 850,\n",
       " 'evolv': 851,\n",
       " 'exact': 852,\n",
       " 'examin': 853,\n",
       " 'examincd': 854,\n",
       " 'exampl': 855,\n",
       " 'excel': 856,\n",
       " 'excess': 857,\n",
       " 'exchang': 858,\n",
       " 'excit': 859,\n",
       " 'exclud': 860,\n",
       " 'exercis': 861,\n",
       " 'exert': 862,\n",
       " 'exhibit': 863,\n",
       " 'exist': 864,\n",
       " 'expand': 865,\n",
       " 'expect': 866,\n",
       " 'experi': 867,\n",
       " 'experienc': 868,\n",
       " 'experienti': 869,\n",
       " 'experiment': 870,\n",
       " 'explain': 871,\n",
       " 'explanatori': 872,\n",
       " 'explicit': 873,\n",
       " 'exploit': 874,\n",
       " 'explos': 875,\n",
       " 'expos': 876,\n",
       " 'exposur': 877,\n",
       " 'express': 878,\n",
       " 'extend': 879,\n",
       " 'extens': 880,\n",
       " 'extent': 881,\n",
       " 'extern': 882,\n",
       " 'extra': 883,\n",
       " 'extracellular': 884,\n",
       " 'extract': 885,\n",
       " 'extrem': 886,\n",
       " 'extrins': 887,\n",
       " 'fabric': 888,\n",
       " 'facilit': 889,\n",
       " 'factor': 890,\n",
       " 'faculti': 891,\n",
       " 'fahrungen': 892,\n",
       " 'fail': 893,\n",
       " 'failur': 894,\n",
       " 'fair': 895,\n",
       " 'falciparum': 896,\n",
       " 'fall': 897,\n",
       " 'fals': 898,\n",
       " 'famili': 899,\n",
       " 'far': 900,\n",
       " 'farnesen': 901,\n",
       " 'farnesol': 902,\n",
       " 'fasassoci': 903,\n",
       " 'faster': 904,\n",
       " 'fastest': 905,\n",
       " 'fcc': 906,\n",
       " 'featur': 907,\n",
       " 'februari': 908,\n",
       " 'fed': 909,\n",
       " 'femal': 910,\n",
       " 'fermi': 911,\n",
       " 'fermion': 912,\n",
       " 'ferromagnet': 913,\n",
       " 'festsetzung': 914,\n",
       " 'feulgendna': 915,\n",
       " 'fever': 916,\n",
       " 'fewer': 917,\n",
       " 'fewest': 918,\n",
       " 'fft': 919,\n",
       " 'fftifft': 920,\n",
       " 'fiber': 921,\n",
       " 'fiberzeugend': 922,\n",
       " 'fiblichen': 923,\n",
       " 'fibroblast': 924,\n",
       " 'field': 925,\n",
       " 'fifteen': 926,\n",
       " 'fifti': 927,\n",
       " 'figur': 928,\n",
       " 'fill': 929,\n",
       " 'filler': 930,\n",
       " 'film': 931,\n",
       " 'filter': 932,\n",
       " 'final': 933,\n",
       " 'financi': 934,\n",
       " 'find': 935,\n",
       " 'fine': 936,\n",
       " 'finger': 937,\n",
       " 'firewal': 938,\n",
       " 'first': 939,\n",
       " 'firstprincipl': 940,\n",
       " 'fisheri': 941,\n",
       " 'five': 942,\n",
       " 'fix': 943,\n",
       " 'flayer': 944,\n",
       " 'flexibl': 945,\n",
       " 'flice': 946,\n",
       " 'flight': 947,\n",
       " 'flow': 948,\n",
       " 'floyd': 949,\n",
       " 'fluid': 950,\n",
       " 'fluoresc': 951,\n",
       " 'fmediat': 952,\n",
       " 'fock': 953,\n",
       " 'focus': 954,\n",
       " 'follicl': 955,\n",
       " 'follow': 956,\n",
       " 'followup': 957,\n",
       " 'foot': 958,\n",
       " 'forecast': 959,\n",
       " 'forego': 960,\n",
       " 'form': 961,\n",
       " 'formal': 962,\n",
       " 'format': 963,\n",
       " 'former': 964,\n",
       " 'formul': 965,\n",
       " 'formula': 966,\n",
       " 'found': 967,\n",
       " 'foundat': 968,\n",
       " 'four': 969,\n",
       " 'fourier': 970,\n",
       " 'fourstag': 971,\n",
       " 'fourth': 972,\n",
       " 'fraction': 973,\n",
       " 'fragment': 974,\n",
       " 'framework': 975,\n",
       " 'free': 976,\n",
       " 'freezer': 977,\n",
       " 'frequenc': 978,\n",
       " 'frequent': 979,\n",
       " 'fresh': 980,\n",
       " 'friction': 981,\n",
       " 'fridrich': 982,\n",
       " 'fring': 983,\n",
       " 'fsh': 984,\n",
       " 'ftest': 985,\n",
       " 'ftom': 986,\n",
       " 'full': 987,\n",
       " 'fulli': 988,\n",
       " 'function': 989,\n",
       " 'fundament': 990,\n",
       " 'furthermor': 991,\n",
       " 'futur': 992,\n",
       " 'gain': 993,\n",
       " 'galeus': 994,\n",
       " 'gallat': 995,\n",
       " 'gap': 996,\n",
       " 'gastric': 997,\n",
       " 'gaussian': 998,\n",
       " 'ge': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabAll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = [load(\"matrix/vocabulary/vocab__%04d__\"%ind) for ind in range(nbPartitions)] \n",
    "docs = [load(\"matrix/docsMap/docs__%04d__\"%ind) for ind in range(nbPartitions)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docsAll ={}\n",
    "nbDocs = []\n",
    "for  ind in range(nbPartitions):\n",
    "    d = load(\"matrix/docsMap/docs__%04d__\"%ind)\n",
    "    docsAll.update(d)\n",
    "    nbDocs.append(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 13, 24, 20]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(ind, part):\n",
    "    for x in part :\n",
    "        el = x[1]\n",
    "        yield (docs[ind][el[0]], np.array([vocabs[x[0]][w][0] for w in el[1]]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = corpus2.mapPartitionsWithIndex(encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbTopics = 6\n",
    "nbVocab = len(vocabAll)\n",
    "mainPath = \"/home/nerk/Documents/3A_ENSAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setNullDocCounts(idPartition):\n",
    "#     countDocs = np.zeros((nbDocs, nbTopics))\n",
    "#     file = mainPath + \"/mapReduceLda/matrix/countDocs/countDocs__%d__\"%idPartition\n",
    "#     with open(file, \"wb\") as f :\n",
    "#         pkl.dump(countDocs,f)\n",
    "        \n",
    "# def setNullWordCounts():\n",
    "#     file = mainPath + \"/mapReduceLda/matrix/countWords/countWords\"\n",
    "#     countWords = np.zeros((nbVocab, nbTopics))\n",
    "#     with open(file, \"wb\") as f :\n",
    "#         pkl.dump(countWords,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPartitionId(ind, part):\n",
    "    for el in part :\n",
    "        pass\n",
    "    return [ind]\n",
    "\n",
    "def getPartDocsVocab(ind, it):\n",
    "    pVocab = np.empty(0)\n",
    "    c = 0\n",
    "    for el in it :\n",
    "        pVocab = np.union1d(pVocab, el[1])\n",
    "        c += 1\n",
    "    return  [(ind,c,len(pVocab))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 21, 1044), (1, 13, 732), (2, 24, 1098), (3, 20, 1008)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PartDocsVocab = corpus2.mapPartitionsWithIndex(getPartDocsVocab).collect()\n",
    "PartDocsVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 13, 24, 20]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbDocs = [v[1] for v in PartDocsVocab]\n",
    "nbDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1044, 732, 1098, 1008]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbVocabs = [v[2] for v in PartDocsVocab]\n",
    "nbVocabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordCounts():\n",
    "    file =  \"matrix/countWords/countWords\"\n",
    "    with open(file, \"rb\") as f :\n",
    "        countWords = pkl.load(f)\n",
    "    return countWords\n",
    "\n",
    "def loadDocCounts(nbPartition):\n",
    "    file = \"matrix/countDocs/countDocs__%d__\"%nbPartition\n",
    "    with open(file, \"rb\") as f :\n",
    "        countDocs = pkl.load(f)\n",
    "    return countDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpTop = corpus2.map(lambda x : (x[0], x[1], np.random.choice(nbTopics, len(x[1]) ))).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, array([ 762,  342,  668,  534,  784,   41,  602,  140,  459,  323,  634,\n",
       "          691,  359,  453,   15,  925,  390,  812,  502,  791,  343,  265,\n",
       "          310,  856,  442,  457,  825,  867,  317, 1023,  932,  690,   39,\n",
       "          682,  578,  761,  421,    5,  702]), array([0, 0, 3, 4, 0, 3, 3, 2, 4, 4, 5, 3, 2, 0, 2, 1, 5, 0, 4, 5, 0, 5,\n",
       "         2, 2, 0, 0, 1, 2, 1, 2, 4, 5, 5, 0, 5, 2, 0, 0, 2]))]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.mapPartitionsWithIndex(saveByPartition).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 13, 24, 20]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initDocCounts(ind, part):\n",
    "    # CorpTop must be read from file according to `idPartition`\n",
    "    countDocs = np.zeros((nbDocs[ind], nbTopics))\n",
    "    for el in part :\n",
    "        count = np.bincount(el[2], minlength=nbTopics)\n",
    "        countDocs[el[0]] = count\n",
    "        \n",
    "    file =  \"matrix/countDocs/docs__%04d__\"%ind\n",
    "    with open(file, \"wb\") as f :\n",
    "        pkl.dump(countDocs,f)\n",
    "    return []\n",
    "\n",
    "\n",
    "def initWordCounts(ind, part):\n",
    "    # CorpTop must be read from file according to `idPartition`\n",
    "    countWords = np.zeros((nbVocabs[ind], nbTopics))\n",
    "    \n",
    "    for el in part :\n",
    "        countWords[el[1], el[2]] += 1\n",
    "            \n",
    "    file = \"matrix/countWords/words__%04d__\"%ind\n",
    "    with open(file, \"wb\") as f :\n",
    "        pkl.dump(countWords,f)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.mapPartitionsWithIndex(initDocCounts).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.mapPartitionsWithIndex(initWordCounts).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "initCountWordsAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 2., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5170"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = corpus.flatMap(lambda x : [(w, 1) for w in x[1]]).countByKey()\n",
    "sum(freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert int(countWords.sum()) == sum(freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updateCountWordsAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 2., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = [ 104,  168,  588,  723,  990,  998, 1764, 1907, 2140, 2315, 2363,\n",
    "        2364, 2484, 2573, 2856, 3037, 3059]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 13, 24, 20]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpTop.glom().map(len).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 100\n",
    "beta = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pldaMap0(ind, part):\n",
    "    countWords = load(\"matrix/countWords/words__%04d__\"%ind)\n",
    "    countDocs = load(\"matrix/countDocs/docs__%04d__\"%ind)\n",
    "    deltaWords = np.zeros(countWords.shape)\n",
    "    sumWordTopics = countWords.sum(0)\n",
    "    ndocs = len(countDocs)\n",
    "    i = 0\n",
    "    for el in part :\n",
    "        cDoc = countDocs[el[0]]\n",
    "        tnews = []\n",
    "        for w,t in zip(el[1], el[2]) :\n",
    "            cDoc[t] -= 1\n",
    "            deltaWords[w,t] -= 1\n",
    "            countWords[w,t] -= 1\n",
    "            sumWordTopics[t] -= 1\n",
    "\n",
    "            proba = (cDoc + alpha)*(countWords[w] + beta)/(sumWordTopics + nbVocab*beta)\n",
    "        \n",
    "            tnew = np.random.choice(nbTopics, p =proba/proba.sum())\n",
    "            \n",
    "            cDoc[tnew] += 1\n",
    "            deltaWords[w,tnew] += 1\n",
    "            countWords[w,tnew] += 1\n",
    "            sumWordTopics[tnew] += 1\n",
    "            \n",
    "            tnews.append(tnew)\n",
    "            \n",
    "        countDocs[el[0],:] = cDoc\n",
    "        i += 1\n",
    "#         if i == ndocs :\n",
    "#             #dump(countDocs, \"matrix/countDocs/docs__%04d__\"%ind )\n",
    "#             dump(deltaWords, \"matrix/deltaWords/deltas__%04d__\"%ind )\n",
    "        yield (el[0],el[1], np.array(tnews))\n",
    "    \n",
    "    dump(deltaWords, \"matrix/deltaWords/deltas__%04d__\"%ind )\n",
    "\n",
    "            \n",
    "#     return [(ind, countWords)] #[(ind, deltaWords, countWords)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleLoader(pklFile):\n",
    "    try:\n",
    "        while True:\n",
    "            yield pkl.load(pklFile)\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plda_one(batchstr, countDocs, countWords,deltaWords, sumWordTopics):\n",
    "    temp = batchstr + \"__temp__\"\n",
    "    with open(batchstr, \"rb\" ) as f1 :\n",
    "        with open(temp, \"wb\") as f2 :\n",
    "            try:\n",
    "                for el in pickleLoader(f1) :\n",
    "                    cDoc = countDocs[el[0]]\n",
    "                    tnews = []\n",
    "                    for w,t in zip(el[1], el[2]) :\n",
    "                        cDoc[t] -= 1\n",
    "                        deltaWords[w,t] -= 1\n",
    "                        countWords[w,t] -= 1\n",
    "                        sumWordTopics[t] -= 1\n",
    "\n",
    "                        proba = (cDoc + alpha)*(countWords[w] + beta)/(sumWordTopics + nbVocab*beta)\n",
    "\n",
    "                        tnew = np.random.choice(nbTopics, p =proba/proba.sum())\n",
    "\n",
    "                        cDoc[tnew] += 1\n",
    "                        deltaWords[w,tnew] += 1\n",
    "                        countWords[w,tnew] += 1\n",
    "                        sumWordTopics[tnew] += 1\n",
    "\n",
    "                        tnews.append(tnew)\n",
    "\n",
    "                    countDocs[el[0],:] = cDoc\n",
    "                    pkl.dump((el[0],el[1], np.array(tnews)), f2)\n",
    "            except  :\n",
    "#                 os.remove(temp)\n",
    "                raise ValueError(\"************  jjjj  Something went wrong in plda_one\")\n",
    "                    \n",
    "    os.remove(batchstr)\n",
    "    os.rename(temp, batchstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"a\": 1, \"b\": [1, 2]}'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps({\"a\": 1, \"b\":[1,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeConfig( **kwargs):\n",
    "    id_ = kwargs[\"id\"]\n",
    "    print(id_)\n",
    "    if isinstance(id_,int):\n",
    "        file = \"configs/config__%04d__\"%id_\n",
    "    else :\n",
    "        file = \"configs/config__all__\"\n",
    "#     print(json.dumps(kwargs))\n",
    "    with open(file, \"w\") as f :\n",
    "        json.dump(kwargs, f)\n",
    "        \n",
    "def updateConfig( **kwargs):\n",
    "    id_ = kwargs[\"id\"]\n",
    "    if isinstance(id_,int):\n",
    "        file = \"configs/config__%04d__\"%id_\n",
    "    else :\n",
    "        file = \"configs/config__all__\"\n",
    "        \n",
    "    with open(file, \"r\") as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    config.update(kwargs)\n",
    "    \n",
    "    with open(file, \"w\") as f :\n",
    "        json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_now():\n",
    "    return str(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-02-03 23:58:32.065096'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pldaMap(ind, nrounds):\n",
    "#     params = next(part)\n",
    "#     assert ind == params[0]\n",
    "#     nrounds = params[1]\n",
    "#     ind, nrounds = params\n",
    "    \n",
    "    rounds = 0\n",
    "    while rounds < nrounds :\n",
    "        \n",
    "        makeConfig(id = ind, state = \"busy\", time = get_now())\n",
    "        \n",
    "        countWords = load(\"matrix/countWords/words__%04d__\"%ind)\n",
    "        countDocs = load(\"matrix/countDocs/docs__%04d__\"%ind)\n",
    "        deltaWords = np.zeros(countWords.shape)\n",
    "        sumWordTopics = countWords.sum(0)\n",
    "        ndocs = len(countDocs)\n",
    "        root = \"matrix/corpusTopic/partition__%04d__/\"%ind\n",
    "        files = os.listdir(root)\n",
    "        files = [file for file in files if \"temp\" not in file]\n",
    "\n",
    "        for  file in files :\n",
    "            plda_one(root + file, countDocs, countWords, deltaWords, sumWordTopics)\n",
    "\n",
    "        dump(deltaWords, \"matrix/deltaWords/deltas__%04d__\"%ind )\n",
    "        \n",
    "        rounds += 1\n",
    "        \n",
    "        now = get_now()\n",
    "        updateConfig(id = ind, state = \"free\", time = now)\n",
    "        \n",
    "        \n",
    "        timeout = 60\n",
    "        poll = 1\n",
    "        waited = 0\n",
    "        while waited < timeout :\n",
    "            with open(\"configs/config__all__\", \"r\") as f:\n",
    "                master = json.load(f)\n",
    "                \n",
    "            if master[\"countWordsUpdated\"][str(ind)] and master[\"time\"] >  now:\n",
    "                \n",
    "#                 updateConfig(id = \"all\",countWordsUpdated = False )\n",
    "                break\n",
    "            else:\n",
    "                t0 = time.time()\n",
    "                time.sleep(poll)\n",
    "                waited += time.time() - t0\n",
    "#         time.sleep(poll)\n",
    "        if waited >= timeout :\n",
    "            raise ValueError(\"Timeout : updates last too much to be done, waited %f s\\\n",
    "                             yet now, \"%waited)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = spark.sparkContext.parallelize([(ind, 10) for ind in range(nbPartitions)])\n",
    "# params = params.partitionBy(nbPartitions)\n",
    "# params.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = spark.sparkContext.parallelize([(ind, 10) for ind in range(nbPartitions)])\n",
    "params = params.partitionBy(nbPartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 1., 0., 0., 2., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpTop.mapPartitionsWithIndex(saveByPartition).collect()\n",
    "\n",
    "corpTop.mapPartitionsWithIndex(initDocCounts).collect()\n",
    "corpTop.mapPartitionsWithIndex(initWordCounts).collect()\n",
    "\n",
    "initCountWordsAll()\n",
    "\n",
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n"
     ]
    }
   ],
   "source": [
    "makeConfig(id = \"all\", countWordsUpdated = {str(ind):False for ind in range(nbPartitions)}, time = get_now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervise(nrounds):\n",
    "    \n",
    "    processes = [Process(target= pldaMap, args = (ind, nrounds)) for ind in range(nbPartitions)]\n",
    "    if  __name__ == \"__main__\":\n",
    "        # Run processes\n",
    "        for p in processes:\n",
    "            p.start()\n",
    "    count = 0        \n",
    "    while count < nrounds :\n",
    "        allFree = True\n",
    "        for id_ in range(nbPartitions):\n",
    "            with open(\"configs/config__%04d__\"%id_, \"r\") as f :\n",
    "                slave = json.load(f)\n",
    "                if slave[\"state\"] == \"busy\":\n",
    "                    allFree = False\n",
    "                    \n",
    "        if allFree :\n",
    "            updateCountWordsAll()\n",
    "            updateConfig(id = \"all\", \n",
    "                         countWordsUpdated = {str(ind):True for ind in range(nbPartitions)}, time = get_now() )\n",
    "            count += 1\n",
    "        \n",
    "            \n",
    "        if not all([p.is_alive() for  p in processes]) :\n",
    "            for p in processes :\n",
    "                p.kill() \n",
    "            raise ValueError(\"Some process is died !!!!!!!!!!!!\")\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "master = Process(target = supervise, args = [150] )\n",
    "master.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in processes :\n",
    "#     print(p.is_alive())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 1., 2.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords = load(\"matrix/countWords/words_all\")\n",
    "countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21, 13, 24, 20]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.,  8.,  5.,  7., 10.,  9.],\n",
       "       [19., 19., 10., 18., 13., 17.],\n",
       "       [ 8., 10., 12.,  9.,  9., 11.],\n",
       "       [13., 14., 14., 14., 15., 14.],\n",
       "       [12., 12.,  6.,  9.,  8.,  8.],\n",
       "       [ 5.,  7.,  4.,  5., 12.,  6.],\n",
       "       [20., 17., 13., 14., 12., 15.],\n",
       "       [12., 13., 10., 18., 17., 17.],\n",
       "       [15., 13., 18., 18., 15., 21.],\n",
       "       [11., 10., 15., 18., 13.,  9.]])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = 1\n",
    "subdoc = 2\n",
    "countDocs = load(\"matrix/countDocs/docs__%04d__\"%subdoc)\n",
    "countDocs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = countDocs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.where(topics == cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7., 17.,  4.,  2.,  8.,  7.],\n",
       "       [22., 27., 16., 14., 26., 22.],\n",
       "       [14., 23., 18., 17., 11., 11.],\n",
       "       [11., 13.,  9., 10.,  5., 10.]])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countDocs[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[cl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "dks = np.array( list(docs[subdoc].items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3016c9b9e3aeafe6fc93558316b472953d8b5bbf', '11'],\n",
       "       ['88092fb7437b2f2afd8cd7643a8fd1377d6764fd', '16'],\n",
       "       ['6374215829deb1843e01716dfa815404016602b2', '19'],\n",
       "       ['630cd25ecf943ac9c1c0cec3b862e8cac5875f76', '20']], dtype='<U40')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster = dks[v]\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3016c9b9e3aeafe6fc93558316b472953d8b5bbf',\n",
       "  array(['penetr', 'main', 'account', 'theori', 'accord', 'winter',\n",
       "         'critic', 'lower', 'effect', 'valu', 'invers', 'electron',\n",
       "         'featur', 'follow', 'air', 'depend', 'flayer', 'temperatur',\n",
       "         'success', 'data', 'frequenc', 'height', 'variat', 'elay',\n",
       "         'pressur', 'regular', 'densiti', 'explain', 'sun', 'simpl',\n",
       "         'determin', 'annual', 'summer', 'km', 'wherea', 'noon', 'observ',\n",
       "         'maximum', 'appleton', 'show', 'limit', 'photoion', 'high',\n",
       "         'accumul', 'higher'], dtype='<U10')),\n",
       " ('88092fb7437b2f2afd8cd7643a8fd1377d6764fd',\n",
       "  array(['perfusionbas', 'therefor', 'atherosclerosi', 'rest', 'find',\n",
       "         'avail', 'addit', 'viabl', 'emiss', 'though', 'definit',\n",
       "         'prognost', 'ischem', 'furthermor', 'often', 'anatom', 'may',\n",
       "         'process', 'appear', 'comput', 'even', 'mild', 'threshold',\n",
       "         'random', 'limit', 'individu', 'poor', 'end', 'result', 'moder',\n",
       "         'suffici', 'ct', 'ischemia', 'blood', 'forego', 'exercis',\n",
       "         'singlephoton', 'magnet', 'studi', 'overestim', 'make', 'diagnost',\n",
       "         'myocardium', 'defect', 'negat', 'contrast', 'use', 'unfortun',\n",
       "         'capac', 'patient', 'evalu', 'electrocardiographi', 'remark',\n",
       "         'diagnos', 'reflect', 'retrospect', 'higher', 'spect', 'accuraci',\n",
       "         'fewer', 'flow', 'inappropri', 'power', 'lack', 'particular',\n",
       "         'revers', 'combin', 'anatomybas', 'reduc', 'likelihood', 'decis',\n",
       "         'diseas', 'coronari', 'left', 'tomographi', 'valuabl',\n",
       "         'wellexecut', 'imag', 'identifi', 'low', 'test', 'pretest',\n",
       "         'reserv', 'best', 'benefit', 'mri', 'need', 'revascular',\n",
       "         'angiographi', 'pure', 'stenosi', 'least', 'respect', 'sever',\n",
       "         'systol', 'abnorm', 'reson', 'function', 'noninvas', 'perform',\n",
       "         'valu', 'ventricular', 'mpi', 'cad', 'exclud', 'probabl',\n",
       "         'intermedi', 'similar', 'regard', 'invas', 'radionuclid', 'correl',\n",
       "         'good', 'myocardi', 'pet', 'cardiac', 'arteri', 'offer', 'risk',\n",
       "         'mani', 'eventu', 'assess', 'perfus', 'lead', 'high', 'trial',\n",
       "         'predict'], dtype='<U19')),\n",
       " ('6374215829deb1843e01716dfa815404016602b2',\n",
       "  array(['level', 'ht', 'broad', 'analges', 'uniqu', 'compar', 'includ',\n",
       "         'avail', 'improv', 'addit', 'content', 'option', 'current', 'ne',\n",
       "         'furthermor', 'gastric', 'painassoci', 'serotonin', 'treatment',\n",
       "         'amelior', 'extracellular', 'may', 'empti', 'duloxetin', 'rat',\n",
       "         'injuri', 'result', 'inhibit', 'metabolit', 'cord', 'hand',\n",
       "         'effect', 'studi', 'behavior', 'model', 'elev', 'mechan',\n",
       "         'allodynia', 'monoamin', 'investig', 'use', 'patient',\n",
       "         'monobenzenesulfon', 'thermal', 'weaker', 'cciinduc', 'clinic',\n",
       "         'constrict', 'vitro', 'reduc', 'antidepress', 'painrel', 'differ',\n",
       "         'spontan', 'diseas', 'tend', 'ratio', 'hyperalgesia', 'dorsal',\n",
       "         'increas', 'amitriptylin', 'turnov', 'spinal',\n",
       "         'rhindenyloxymethylmorpholin', 'neuropath', 'much', 'indeloxazin',\n",
       "         'characterist', 'cci', 'although', 'suggest', 'lower', 'potenti',\n",
       "         'pharmacolog', 'indic', 'profil', 'type', 'isom', 'reuptak',\n",
       "         'chronic', 'signific', 'action', 'pain', 'horn', 'norepinephrin',\n",
       "         'cerebrovascular', 'dose', 'various', 'wherea', 'analgesia',\n",
       "         'slight', 'exert', 'affect', 'multipl'], dtype='<U27')),\n",
       " ('630cd25ecf943ac9c1c0cec3b862e8cac5875f76',\n",
       "  array(['result', 'cach', 'block', 'crucial', 'industri', 'sever',\n",
       "         'closur', 'schedul', 'remov', 'case', 'analysi', 'parallel',\n",
       "         'point', 'processor', 'perform', 'multicor', 'first', 'implement',\n",
       "         'algorithm', 'bottleneck', 'warshal', 'cell', 'core', 'transit',\n",
       "         'possibl', 'communic', 'experiment', 'data', 'approach', 'gflop',\n",
       "         'memori', 'mechan', 'mappingmethodolog', 'determin', 'solv', 'use',\n",
       "         'also', 'optim', 'order', 'present', 'defin', 'peak',\n",
       "         'selfschedul', 'scalabl', 'near', 'standard', 'applic', 'synchron',\n",
       "         'design', 'floyd', 'version', 'comput', 'play', 'scientif',\n",
       "         'paper', 'show', 'role', 'achiev'], dtype='<U17'))]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.filter(lambda x : np.isin(x[0], cluster[:, 0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = data.map(lambda x : json.loads(x)).filter(lambda x : np.isin(x[\"id\"], cluster[:,0])).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entities': ['Electron', 'Tree accumulation', 'anatomical layer'],\n",
       "  'journalVolume': '139',\n",
       "  'journalPages': '328-329',\n",
       "  'pmid': '',\n",
       "  'year': 1937,\n",
       "  'outCitations': [],\n",
       "  's2Url': 'https://semanticscholar.org/paper/3016c9b9e3aeafe6fc93558316b472953d8b5bbf',\n",
       "  's2PdfUrl': '',\n",
       "  'id': '3016c9b9e3aeafe6fc93558316b472953d8b5bbf',\n",
       "  'authors': [{'name': 'Leiv Harang', 'ids': ['49288990']}],\n",
       "  'journalName': 'Nature',\n",
       "  'paperAbstract': 'THE successive accumulation of data on the critical frequencies, that is, the limiting penetrating frequencies, of the E- and F-layers determined from noon observations show the following main features1: the critical frequencies of the E-layer show a regular annual variation depending on the height of the sun, whereas the critical frequencies of the F2-layer show an inverse annual variation, with high values during winter and lower values in summer. Appleton2 has explained this as a temperature effect, the density of the air at 200–400 km. in winter being higher on account of the lower temperature. According to the simple theory of photo-ionization, the maximum electron density, that is, the critical frequency, will depend on the pressure.',\n",
       "  'inCitations': [],\n",
       "  'pdfUrls': [],\n",
       "  'title': 'Annual Variation of the Critical Frequencies of the E- and F2-Layers',\n",
       "  'doi': '10.1038/139328b0',\n",
       "  'sources': [],\n",
       "  'doiUrl': 'https://doi.org/10.1038/139328b0',\n",
       "  'venue': 'Nature'},\n",
       " {'entities': ['Anatomic structures',\n",
       "   'Arteriopathic disease',\n",
       "   'Atherosclerosis',\n",
       "   'Biologic Preservation',\n",
       "   'Coronary Artery Disease',\n",
       "   'Decision Making',\n",
       "   'Ejection fraction (procedure)',\n",
       "   'Forecast of outcome',\n",
       "   'Hematological Disease',\n",
       "   'Increment',\n",
       "   'Microvascular Network',\n",
       "   'Moderate Response',\n",
       "   'Myocardial Blood Flow',\n",
       "   'Myocardial Perfusion Imaging',\n",
       "   'Myocardium',\n",
       "   'Negative Predictive Value of Diagnostic Test',\n",
       "   'Neural Tube Defects',\n",
       "   'PCYT1A wt Allele',\n",
       "   'PET/CT scan',\n",
       "   'Patients',\n",
       "   'Photons',\n",
       "   'Positron-Emission Tomography',\n",
       "   'Radionuclide Imaging',\n",
       "   'Rest',\n",
       "   'SLPI protein, human',\n",
       "   'Stratification',\n",
       "   'Tomography, Emission-Computed',\n",
       "   'Tomography, Emission-Computed, Single-Photon',\n",
       "   'X-Ray Computed Tomography',\n",
       "   'angiogram',\n",
       "   'revascularization'],\n",
       "  'journalVolume': '44 4',\n",
       "  'journalPages': '\\n          320-9\\n        ',\n",
       "  'pmid': '24948154v1',\n",
       "  'year': 2014,\n",
       "  'outCitations': ['c109261514ad776a6b2c705766ad625908989d94',\n",
       "   'dbb1548cf04e16862668160feefb0421f24558a1',\n",
       "   '06419604c464610c8c18fa928f779af197dc9b5c',\n",
       "   'efa6a37d2d2f23ca9a787192b363b33653125419',\n",
       "   '00ff1b20f34e22541394bf862814a02773ff3a7c',\n",
       "   '3675df44c6fe98c6e77645958199224a86cb6e9f',\n",
       "   '57e5eb5bcd347ae4d62b34ec4dd99dd657fe0227',\n",
       "   'e914d841c48a440fcbe46f6fc11cf644ec54be61',\n",
       "   '97fd252a1e88ed1ee4cc6c9172424adcd8190317',\n",
       "   'fc1cfb9975c505108ae543a4da3f1708d241d952',\n",
       "   'f73e77141941c65599335a16d5da56385aa212ab',\n",
       "   '65fcfeae894ac4508d7e8dc9074330eed22f0fa6',\n",
       "   '21e20b0737f29e9c944c99f5b3a32f23eef8126a',\n",
       "   'a164a1eddc7292b8335a780139d524e1c7498476',\n",
       "   '7433a7bfbb11e1b425fe4e65a06326a3922a3069',\n",
       "   'ace5ae2de92e2494913da0796734f5ef163ea580',\n",
       "   'ea76be97d3485ebeb91fc5273f08f8592468bd10',\n",
       "   'dacbe2faf9e7d722ae1308c3cf460381874f12b5',\n",
       "   'b40c260bcae9812a44f7b288161a542bad337ed1',\n",
       "   '3225249caa04d0fd2603049dd7ac8bf465c38943',\n",
       "   '3ff6a11da0500931f99b406dcac27dbd58a72fac',\n",
       "   '93d87a773cd77b4560bcd597623e6e193a0cb66a',\n",
       "   '79c5477ef9e2b86188b034423862acb5db3e7086',\n",
       "   '0f26da9e575ca2c34d2c597fcb9bfc378d569fc1',\n",
       "   '92f664796955d316f6051c433478fdc8972fce28',\n",
       "   '7c9f74e7b719ef5abb45d5fb6dd7e01c465009aa',\n",
       "   'fbd2fe18705d11ab585e993bed273f4e25fa2100',\n",
       "   'deec45b8d87426331948c362814cf727a41a1803',\n",
       "   'd5bf279c2110781c13ee3ecd7da2b37f966e7af9',\n",
       "   'f41584a89d030872176182ada161d158b62e4582',\n",
       "   'a0658d3b8a3357b9bdeb30f0569b54dcd43086c8',\n",
       "   '87697fb56f9614e9f80212686ec3871903f98c42',\n",
       "   '56382db94b5ff1f935be684834f63116635d1f3d',\n",
       "   '2eb9273d9d6257fbb62eb11f0819eae69a3dd366',\n",
       "   '0a66fd3891336fe1ed8b5a3a44e5c749deb6e72f',\n",
       "   '3718d67cf761ade39ea9d7fbc413ef47fa32b61b',\n",
       "   'b3df5fb80993cedbc6f589ac444d7fcc25d89bca',\n",
       "   '0a41db4609240120c1a906e6cf2e6b66a26e0965',\n",
       "   '1658e3a51828b52c154c93af1b6cec9e1398695d',\n",
       "   '9812a0a61a0bea0b34c8553e5b441ab80ce7f4b2',\n",
       "   '929f3bb5e7407bca136a8626530e2f125096303b',\n",
       "   'a5b004bd6b9019c2b352aa38579b26e163fc7efd',\n",
       "   '3056c4fc8d1aad2afaa40a73895265c06d769786',\n",
       "   '51c588399faa2905908b5eb92063677de2675543',\n",
       "   'b90dd21663989a7f5881f62bf82baf46b98afb30',\n",
       "   '6a6c63cf2b5459af744b7541c4da8d915f0a88bc',\n",
       "   '9327956a80bad54aef42b35183c1f476b4902bbb',\n",
       "   '20c728a45c708bcd1101009f2039d0a0181db1f4',\n",
       "   '411186330875c3461c3e730ef148e0906f683d06',\n",
       "   '2ed821d04c17d0915c985ea7d85b1fdf07935e7a',\n",
       "   'adda39e62563f6d5c18a546d86d1031947504594',\n",
       "   '2b755f1d44a2fecdd99b21eacd986ed42101b2de',\n",
       "   '1e5a92e496fe85b8be7ecd9be9554f7009462d7f',\n",
       "   '8425f8ab6e590d84ac14ca2e2cbe89e114a3e47e',\n",
       "   'f0c8f3757e8cf78d518f01045c48b09be8fbf3fb',\n",
       "   'a782cbe4564f46cb6419256fdb0adcad61c9ae27',\n",
       "   '21088997418ef7f31b3627763f6d2ed248ed1586',\n",
       "   '72e46654b770367715dff555b73150d858540896',\n",
       "   '2663a16b3e9e743528fe7d44951ef95636b8601d',\n",
       "   '20d93bee70ab319a2a9f0d45f45a7de331d2b2fd',\n",
       "   '3e3aa95f158cca391d5e0444fa237c5ebbd4ba61'],\n",
       "  's2Url': 'https://semanticscholar.org/paper/88092fb7437b2f2afd8cd7643a8fd1377d6764fd',\n",
       "  's2PdfUrl': '',\n",
       "  'id': '88092fb7437b2f2afd8cd7643a8fd1377d6764fd',\n",
       "  'authors': [{'name': 'Paul C. Cremer', 'ids': ['35221063']},\n",
       "   {'name': 'Rory Hachamovitch', 'ids': ['6912258']},\n",
       "   {'name': 'Balaji Tamarappoo', 'ids': ['3106562']}],\n",
       "  'journalName': 'Seminars in nuclear medicine',\n",
       "  'paperAbstract': 'Myocardial perfusion imaging (MPI) to diagnose coronary artery disease (CAD) is best performed in patients with intermediate pretest likelihood of disease; unfortunately, pretest likelihood is often overestimated, resulting in the inappropriate use of perfusion imaging. A good functional capacity often predicts low risk, and MPI for diagnosing CAD should be reserved for individuals with poor exercise capacity, abnormal resting electrocardiography, or an intermediate or high probability of CAD. With respect to anatomy-based testing, coronary CT angiography has a good negative predictive value, but stenosis severity correlates poorly with ischemia. Therefore decision making with respect to revascularization may be limited when a purely noninvasive anatomical test is used. Regarding perfusion imaging, the diagnostic accuracies of SPECT, PET, and cardiac magnetic resonance are similar, though fewer studies are available with cardiac magnetic resonance. PET coronary flow reserve may offer a negative predictive value sufficiently high to exclude severe CAD such that patients with mild to moderate reversible perfusion defects can forego invasive angiography. In addition, combined anatomical and perfusion-based imaging may eventually offer a definitive evaluation for diagnosing CAD, even in higher risk patients. Any remarkable findings on single-photon emission computed tomography and PET MPI studies are valuable for prognostication. Furthermore, assessment of myocardial blood flow with PET is particularly powerful for prognostication as it reflects the end result of many processes that lead to atherosclerosis. Decision making with respect to revascularization is limited for cardiac MRI and PET MPI. In contrast, retrospective radionuclide studies have identified an ischemic threshold, but randomized trials are needed. In patients with at least moderately reduced left ventricular systolic function, viable myocardium as assessed by PET or MRI, appears to identify patients who benefit from revascularization, but well-executed randomized trials are lacking.',\n",
       "  'inCitations': ['d17a9ab7bb37aeac062ec387d609a945cb9b5164',\n",
       "   '850e70fa8d4d7d0eb0dd62182127d6d241005590',\n",
       "   '9ad754d474f5c543dd862b309d21d20099d351ee',\n",
       "   '90bdbc93d5a9ccbfec9a738a130e9d5925bc9dcf',\n",
       "   '8cbc2c1bdd5cd2288c3292364ed267f77d3a3f91',\n",
       "   '67c3971c99f9a96e95639fb0b6c48dd8484f0c6e',\n",
       "   'c070346181e18cf9868a11bed6fb7dc0153bfaaa',\n",
       "   '8794750ef1f33b9e560b77385a66bd5cdf52929a'],\n",
       "  'pdfUrls': [],\n",
       "  'title': 'Clinical decision making with myocardial perfusion imaging in patients with known or suspected coronary artery disease.',\n",
       "  'doi': '10.1053/j.semnuclmed.2014.04.006',\n",
       "  'sources': ['Medline'],\n",
       "  'doiUrl': 'https://doi.org/10.1053/j.semnuclmed.2014.04.006',\n",
       "  'venue': 'Seminars in nuclear medicine'},\n",
       " {'entities': ['AS1069562',\n",
       "   'Amitriptyline',\n",
       "   'Analgesics',\n",
       "   'Antidepressive Agents',\n",
       "   'Cerebrovascular Disorders',\n",
       "   'Gastric Emptying',\n",
       "   'Gastroparesis',\n",
       "   'Hyperalgesia',\n",
       "   'In Vitro [Publication Type]',\n",
       "   'Metabolite',\n",
       "   'Neuralgia',\n",
       "   'Norepinephrine',\n",
       "   'Pain',\n",
       "   'Patients',\n",
       "   'Pharmacology',\n",
       "   'Serotonin',\n",
       "   'Structure of posterior gray horn of spinal cord',\n",
       "   'XP 280',\n",
       "   'bepotastine',\n",
       "   'contents - HtmlLinkType',\n",
       "   'duloxetine',\n",
       "   'indeloxazine',\n",
       "   'monoamine',\n",
       "   'physical hard work'],\n",
       "  'journalVolume': '348 3',\n",
       "  'journalPages': '\\n          372-82\\n        ',\n",
       "  'pmid': '24338505v1',\n",
       "  'year': 2014,\n",
       "  'outCitations': ['8c8cf14e66e193b7683003c7262855303f4dd229',\n",
       "   '1060c181909ba66b25db064ae4640e8588e26dc6',\n",
       "   'c9df759f84640327b344e23274a424bfe65b6e60',\n",
       "   '464e2bd0a79e5e00d341757c6a40073f4e28955f',\n",
       "   '8dc0090b134bf99155ed1c0645d79a75995ea728',\n",
       "   'c83db72982634aeceefa503fb1d35a5f83fe67e4',\n",
       "   '9571b7982bd32dd9ff7520dd7901023354e511be',\n",
       "   'fdbdc8b8060fa28f13f411a5c48a9564277fa0e8',\n",
       "   '613b35802d082019d982d29c2e2ee029108b8459',\n",
       "   '88babf41f95c68f7e169544bdf8257dbdfa17b78',\n",
       "   '9221829f865306fab02274dacf71189eb2a4f320',\n",
       "   '3dba3a5ea6f5e4608f9c210f52ff88b69914b9dc',\n",
       "   '4fcf960c6a571252e7ffeac2d97c04ee5ea5a5d7',\n",
       "   '2b1ad2d7173cbb2ca1896c66b83660894fa55e40',\n",
       "   '3616de08d9bdf7dcd293bcf3fefc970aff2ec34d',\n",
       "   '5223f62e2188902699d17ea1fc6eb72063f7f095',\n",
       "   '717815853a82cecb3730b951095a314a5c403b2a',\n",
       "   '299ca5f50c950b1453ac3f9cd34ed77659ecf384',\n",
       "   '9303f3fc73e89d822e7f3f7b256bb0d49c6545c9',\n",
       "   'adf64a6e24a1570c45feae8688b020fe160afae9',\n",
       "   '62a37ef245d06bfccf987ff1254d51093428064d',\n",
       "   'ca37d456b138d5d7ea97b4da5615b61cb498c847',\n",
       "   '9df02f0ea9b5c0948db385d46831652a6083538b',\n",
       "   '095a58597ccc1a2bcecc43de01000f39dc941a85',\n",
       "   '4d85dcbc0b9a6df29bb6a7fe231fd212da53dd94',\n",
       "   '44eeb04410e0da9d9c5532f947601555482aaa61',\n",
       "   '2c925b291cc6fb7cc862796de6a119ee008873ab',\n",
       "   '9692aa7adc46745b2c3b61a54547b06129115bb7'],\n",
       "  's2Url': 'https://semanticscholar.org/paper/6374215829deb1843e01716dfa815404016602b2',\n",
       "  's2PdfUrl': 'http://pdfs.semanticscholar.org/6374/215829deb1843e01716dfa815404016602b2.pdf',\n",
       "  'id': '6374215829deb1843e01716dfa815404016602b2',\n",
       "  'authors': [{'name': 'Nobuhito Murai', 'ids': ['6440344']},\n",
       "   {'name': 'Toshiaki Aoki', 'ids': ['46949565']},\n",
       "   {'name': 'Seiji Tamura', 'ids': ['48952763']},\n",
       "   {'name': 'Toshihiro Sekizawa', 'ids': ['39498727']},\n",
       "   {'name': 'Shuichiro Kakimoto', 'ids': ['36247619']},\n",
       "   {'name': 'Mina Tsukamoto', 'ids': ['2512320']},\n",
       "   {'name': 'Tomoya Oe', 'ids': ['8058266']},\n",
       "   {'name': 'Ryugo Enomoto', 'ids': ['39024951']},\n",
       "   {'name': 'Nozomu Hamakawa', 'ids': ['47809804']},\n",
       "   {'name': 'Nobuya Matsuoka', 'ids': ['3068960']}],\n",
       "  'journalName': 'The Journal of pharmacology and experimental therapeutics',\n",
       "  'paperAbstract': 'AS1069562 [(R)-2-[(1H-inden-7-yloxy)methyl]morpholine monobenzenesulfonate] is the (+)-isomer of indeloxazine, which had been used clinically for the treatment of cerebrovascular diseases with multiple pharmacological actions, including serotonin (5-HT) and norepinephrine (NE) reuptake inhibition. Here we investigated the analgesic effects of AS1069562 in a rat model of chronic constriction injury (CCI)-induced neuropathic pain and the spinal monoamine turnover. These effects were compared with those of the antidepressants duloxetine and amitriptyline. AS1069562 significantly elevated extracellular 5-HT and NE levels in the rat spinal dorsal horn, although its 5-HT and NE reuptake inhibition was much weaker than that of duloxetine in vitro. In addition, AS1069562 increased the ratio of the contents of both 5-HT and NE to their metabolites in rat spinal cord, whereas duloxetine slightly increased only the ratio of the content of 5-HT to its metabolite. In CCI rats, AS1069562 and duloxetine significantly ameliorated mechanical allodynia, whereas amitriptyline did not. AS1069562 and amitriptyline significantly ameliorated thermal hyperalgesia, and duloxetine tended to ameliorate it. Furthermore, AS1069562, duloxetine, and amitriptyline significantly improved spontaneous pain-associated behavior. In a gastric emptying study, AS1069562 affected gastric emptying at the same dose that exerted analgesia in CCI rats. On the other hand, duloxetine and amitriptyline significantly reduced gastric emptying at lower doses than those that exerted analgesic effects. These results indicate that AS1069562 broadly improved various types of neuropathic pain-related behavior in CCI rats with unique characteristics in spinal monoamine turnover, suggesting that AS1069562 may have potential as a treatment option for patients with neuropathic pain, with a different profile from currently available antidepressants.',\n",
       "  'inCitations': ['cff77ca6133180edc209548b28943a9b08d06b17',\n",
       "   'fb063b9bef5e99521c813f32c94730511be4766b',\n",
       "   '20173e8d5f8f3aea5b7a9bfca7a0964d0196ea08',\n",
       "   'bfd6dd3f02daa1eae7a860f361b47e8ae7883c70',\n",
       "   'cfc99791705bed464ac6d1c2ce3ab3335a2d7f77',\n",
       "   'a00a128919eddf8c354b8770050ff2890c2ecc35'],\n",
       "  'pdfUrls': ['http://jpet.aspetjournals.org/content/jpet/348/3/372.full.pdf'],\n",
       "  'title': 'AS1069562, the (+)-isomer of indeloxazine, exerts analgesic effects in a rat model of neuropathic pain with unique characteristics in spinal monoamine turnover.',\n",
       "  'doi': '10.1124/jpet.113.208686',\n",
       "  'sources': ['Medline'],\n",
       "  'doiUrl': 'https://doi.org/10.1124/jpet.113.208686',\n",
       "  'venue': 'The Journal of pharmacology and experimental therapeutics'},\n",
       " {'entities': ['Cell (microprocessor)',\n",
       "   'Computational science',\n",
       "   'Floyd–Warshall algorithm',\n",
       "   'Multi-core processor',\n",
       "   'Scalability',\n",
       "   'Scheduling (computing)',\n",
       "   'Synchronization (computer science)',\n",
       "   'Transitive closure',\n",
       "   'Von Neumann architecture'],\n",
       "  'journalVolume': '',\n",
       "  'journalPages': '1-11',\n",
       "  'pmid': '',\n",
       "  'year': 2009,\n",
       "  'outCitations': ['52b5cd6d32d87d98d89818ee985ddc26545408d3',\n",
       "   '152bd1936d3685b145595e214c43888aa66f020a',\n",
       "   'bb71f94e48503a0d110ccdcf6ac587b96e242dd8',\n",
       "   '21ad1517d156f3d20b9139c5e58a8d8550e26318',\n",
       "   '70298023b87613c265ce9f586d393d8a9b9c9f0a',\n",
       "   '630444d76e5aa81867344cb11aaddaab8dc8174c',\n",
       "   '8d5cb7f4e8a86fe5572175dda91454ee77342eb9',\n",
       "   '627901629473aa838bc3c856605ede4b641ac760',\n",
       "   'f4896279a7d02f6ce7a097f19994beb5311a5867',\n",
       "   '7bcc53f1baf3358517a602d856192faea9442c91',\n",
       "   '2ec9e421edd5f18b82ff1ab4c03979c5acaaccdd',\n",
       "   '5e7243919c7490ec6eecdc0e45a4a77a2ea8957a',\n",
       "   'fc46ab2595af589a1943255b7c43aa1de683cd3a'],\n",
       "  's2Url': 'https://semanticscholar.org/paper/630cd25ecf943ac9c1c0cec3b862e8cac5875f76',\n",
       "  's2PdfUrl': '',\n",
       "  'id': '630cd25ecf943ac9c1c0cec3b862e8cac5875f76',\n",
       "  'authors': [{'name': 'Sudhir Vinjamuri', 'ids': ['1875014']},\n",
       "   {'name': 'Viktor K. Prasanna', 'ids': ['1728271']}],\n",
       "  'journalName': '2009 IEEE International Symposium on Parallel & Distributed Processing',\n",
       "  'paperAbstract': 'In this paper, we present a mappingmethodology and optimizations for solving transitive closure on the Cell multicore processor. Using our approach, it is possible to achieve near peak performance for transitive closure on the Cell processor. We first parallelize the Standard Floyd Warshall algorithm and show through analysis and experimental results that data communication is a bottleneck for performance and scalability. We parallelize a cache optimized version of Floyd Warshall algorithm to remove the memory bottleneck. As is the case with several scientific computing and industrial applications on a multicore processor, synchronization and scheduling of the cores plays a crucial role in determining the performance of this algorithm. We define a self-scheduling mechanism for the cores of a multicore processor and design a self-scheduler for Blocked Floyd Warshall algorithm on the Cell multicore processor to remove the scheduling bottleneck. We also present optimizations in scheduling order to remove synchronization points. Our implementations achieved up to 78GFLOPS.',\n",
       "  'inCitations': ['0d78303ae47cc438689a0f925f865192c520b184',\n",
       "   '5e6840c0a1cd90459b62fc128659b2e27578fbd4',\n",
       "   '25e6fa06bec35de557410514307516c81a44d08d',\n",
       "   '183f0f85f4f671e229650d12bde76df9a8b5092a'],\n",
       "  'pdfUrls': ['http://doi.ieeecomputersociety.org/10.1109/IPDPS.2009.5161072',\n",
       "   'http://halcyon.usc.edu/~pk/prasannawebsite/papers/sudiripdps.pdf'],\n",
       "  'title': 'Transitive closure on the cell broadband engine: A study on self-scheduling in a multicore processor',\n",
       "  'doi': '10.1109/IPDPS.2009.5161072',\n",
       "  'sources': ['DBLP'],\n",
       "  'doiUrl': 'https://doi.org/10.1109/IPDPS.2009.5161072',\n",
       "  'venue': '2009 IEEE International Symposium on Parallel & Distributed Processing'}]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.any(countWords < 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=float64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countWords[countWords < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # the old plda\n",
    "# corpTop.mapPartitionsWithIndex(saveByPartition).collect()\n",
    "\n",
    "# corpTop.mapPartitionsWithIndex(initDocCounts).collect()\n",
    "# corpTop.mapPartitionsWithIndex(initWordCounts).collect()\n",
    "# initCountWordsAll()\n",
    "# countWords = load(\"matrix/countWords/words_all\")\n",
    "# countWords[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = corpTop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd = rdd.mapPartitionsWithIndex(pldaMap0)\n",
    "# rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# updateCountWordsAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCounts2 = load(\"matrix/countWords/words_all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCounts2[wordCounts2 < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCounts2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordCounts2 == wordCounts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
